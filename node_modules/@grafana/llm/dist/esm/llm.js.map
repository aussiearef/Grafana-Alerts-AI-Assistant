{"version":3,"file":"llm.js","sources":["../../src/llm.ts"],"sourcesContent":["/**\n * LLM API client.\n *\n * This module contains functions used to make requests to the LLM provider API via\n * the Grafana LLM app plugin. That plugin must be installed, enabled and configured\n * in order for these functions to work.\n *\n * The {@link enabled} function can be used to check if the plugin is enabled and configured.\n */\n\nimport {\n  isLiveChannelMessageEvent,\n  LiveChannelAddress,\n  LiveChannelMessageEvent,\n  LiveChannelScope,\n} from '@grafana/data';\nimport { getBackendSrv, getGrafanaLiveSrv, logDebug /* logError */ } from '@grafana/runtime';\n\nimport React, { useEffect, useCallback, useState } from 'react';\nimport { useAsync } from 'react-use';\nimport { pipe, Observable, UnaryFunction, Subscription } from 'rxjs';\nimport { filter, map, scan, takeWhile, tap, toArray } from 'rxjs/operators';\nimport { v4 as uuidv4 } from 'uuid';\n\nimport { LLM_PLUGIN_ID, LLM_PLUGIN_ROUTE, setLLMPluginVersion } from './constants';\nimport { HealthCheckResponse, LLMProviderHealthDetails } from './types';\n\nconst LLM_CHAT_COMPLETIONS_PATH = 'llm/v1/chat/completions';\n\n/** The role of a message's author. */\nexport type Role = 'system' | 'user' | 'assistant' | 'function' | 'tool';\n\n/** A message in a conversation. */\nexport interface Message {\n  /** The role of the message's author. */\n  role: Role;\n\n  /** The contents of the message. content is required for all messages, and may be null for assistant messages with function calls. */\n  content?: string;\n\n  /** The ID of the tool call, if this message is a function call. */\n  tool_call_id?: string;\n\n  /**\n   * The name of the author of this message.\n   *\n   * This is required if role is 'function', and it should be the name of the function whose response is in the content.\n   *\n   * May contain a-z, A-Z, 0-9, and underscores, with a maximum length of 64 characters.\n   */\n  name?: string;\n\n  /**\n   * The name and arguments of a function that should be called, as generated by the model.\n   *\n   * @deprecated Use tool_calls instead.\n   */\n  function_call?: Object;\n\n  /**\n   * The tool calls generated by the model, such as function calls.\n   */\n  tool_calls?: ToolCall[];\n}\n\n\n/** A tool call the model may generate. */\nexport interface ToolCall {\n  id: string;\n  index?: number;\n  type: 'function';\n  function: FunctionCall;\n}\n\n/** A function call generated by the model. */\ninterface FunctionCall {\n  /**\n   * The name of the tool to call.\n   */\n  name: string;\n\n  /**\n   * The arguments to call the function with, as generated by the model in JSON format.\n   *\n   * Note that the model does not always generate valid JSON, and may hallucinate\n   * parameters not defined by your function schema. Validate the arguments in\n   * your code before calling your function.\n   */\n  arguments: string;\n}\n\n\n/** A function the model may generate JSON inputs for. */\nexport interface Function {\n  /**\n   * The name of the function to be called.\n   *\n   * Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.\n   */\n  name: string;\n  /**\n   * A description of what the function does, used by the model to choose when and how to call the function.\n   */\n  description?: string;\n  /*\n   * The parameters the functions accepts, described as a JSON Schema object. See the provider's guide for examples, and the JSON Schema reference for documentation about the format.\n   *\n   * Omitting `parameters` defines a function with an empty parameter list.\n   */\n  parameters?: Object;\n  /**\n   * Whether to enable strict schema adherence when generating the function call.\n   *\n   * If set to true, the model will follow the exact schema defined in the parameters field.\n   * Only a subset of JSON Schema is supported when strict is true.\n   */\n  strict?: boolean;\n}\n\n/**\n * Enum representing abstracted models used by the backend app.\n * @enum {string}\n */\nexport enum Model {\n  BASE = 'base',\n  LARGE = 'large',\n}\n\n/**\n * @deprecated Use {@link Model} instead.\n */\ntype DeprecatedString = string;\n\nexport interface ChatCompletionsRequest {\n  /**\n   * Model abstraction to use. These abstractions are then translated back into specific models based on the users settings.\n   *\n   * If not specified, defaults to `Model.BASE`.\n   */\n  model?: Model | DeprecatedString;\n  /** A list of messages comprising the conversation so far. */\n  messages: Message[];\n  /**\n   * What sampling temperature to use, between 0 and 2.\n   * Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n   *\n   * We generally recommend altering this or top_p but not both.\n   */\n  temperature?: number;\n  /**\n   * An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.\n   * So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n   *\n   * We generally recommend altering this or temperature but not both.\n   */\n  top_p?: number;\n  /**\n   * How many chat completion choices to generate for each input message.\n   */\n  n?: number;\n  /**\n   * Up to 4 sequences where the API will stop generating further tokens.\n   */\n  stop?: string | string[];\n  /**\n   * The maximum number of tokens to generate in the chat completion.\n   *\n   * The total length of input tokens and generated tokens is limited by the model's context length. Example Python code for counting tokens.\n   */\n  max_tokens?: number;\n  /**\n   * Number between -2.0 and 2.0.\n   *\n   * Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\n   */\n  presence_penalty?: number;\n  /**\n   * Number between -2.0 and 2.0.\n   *\n   * Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\n   */\n  frequency_penalty?: number;\n  /**\n   * Modify the likelihood of specified tokens appearing in the completion.\n   *\n   * Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100.\n   * Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model,\n   * but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban\n   * or exclusive selection of the relevant token.\n   */\n  logit_bias?: { [key: string]: number };\n  /**\n   * A unique identifier representing your end-user, which can help monitor and detect abuse.\n   */\n  user?: string;\n\n  /** A list of tools that the model may use. */\n  tools?: Tool[];\n}\n\n/** A tool that the model may use. */\nexport interface Tool {\n  type: 'function';\n  /** The function that the model may use. */\n  function: Function;\n}\n\n/** A completion object from the LLM provider. */\nexport interface Choice {\n  /** The message object generated by the model. */\n  message: Message;\n  /**\n   * The reason the model stopped generating text.\n   *\n   * This may be one of:\n   *  - stop: API returned complete message, or a message terminated by one of the stop sequences provided via the stop parameter\n   *  - length: incomplete model output due to max_tokens parameter or token limit\n   *  - function_call: the model decided to call a function\n   *  - content_filter: omitted content due to a flag from our content filters\n   *  - null: API response still in progress or incomplete\n   */\n  finish_reason: string;\n  /** The index of the completion in the list of choices. */\n  index: number;\n}\n\n/** The usage statistics for a request to the LLM provider. */\nexport interface Usage {\n  /** The number of tokens in the prompt. */\n  prompt_tokens: number;\n  /** The number of tokens in the completion. */\n  completion_tokens: number;\n  /** The total number of tokens. */\n  total_tokens: number;\n}\n\n/** The error response from the Grafana LLM app when trying to call the chat completions API. */\ninterface ChatCompletionsErrorResponse {\n  /** The error message. */\n  error: string;\n}\n\n/** A response from the LLM provider Chat Completions API. */\nexport interface ChatCompletionsResponse<T = Choice> {\n  /** The ID of the request. */\n  id: string;\n  /** The type of object returned (e.g. 'chat.completion'). */\n  object: string;\n  /** The timestamp of the request, as a UNIX timestamp. */\n  created: number;\n  /** The name of the model used to generate the response. */\n  model: string;\n  /** A list of completion objects (only one, unless `n > 1` in the request). */\n  choices: T[];\n  /** The number of tokens used to generate the replies, counting prompt, completion, and total. */\n  usage: Usage;\n}\n\n/** A content message returned from the model. */\nexport interface ContentMessage {\n  /** The content of the message. */\n  content: string;\n  /** The role of the author of this message. */\n  role: Role;\n}\n\n/** A message returned from the model indicating that it is done. */\nexport interface DoneMessage {\n  done: boolean;\n}\n\n/** A function call message returned from the model. */\nexport interface FunctionCallMessage {\n  /** The name of the function to call. */\n  name: string;\n  /** The arguments to the function call. */\n  arguments: any[];\n}\n\n/** A tool calls message returned from the model. */\nexport interface ToolCallsMessage {\n  /** The tool calls generated by the model. */\n  tool_calls: ToolCall[];\n  /** The role of the author of this message. */\n  role: Role;\n}\n\n/**\n * A delta returned from a stream of chat completion responses.\n *\n * In practice this will be either a content message or a function call;\n * done messages are filtered out by the `streamChatCompletions` function.\n */\nexport type ChatCompletionsDelta = ContentMessage | FunctionCallMessage | DoneMessage | ToolCallsMessage;\n\n/** A chunk included in a chat completion response. */\nexport interface ChatCompletionsChunk {\n  /** The delta since the previous chunk. */\n  delta: ChatCompletionsDelta;\n}\n\n/** Return true if the message is a 'content' message. */\nexport function isContentMessage(message: ChatCompletionsDelta): message is ContentMessage {\n  return 'content' in message;\n}\n\n/** Return true if the message is a 'done' message. */\nexport function isDoneMessage(message: ChatCompletionsDelta): message is DoneMessage {\n  return 'done' in message && message.done != null;\n}\n\n/** Return true if the response is an error response. */\nexport function isErrorResponse<T>(\n  response: ChatCompletionsResponse<T> | ChatCompletionsErrorResponse\n): response is ChatCompletionsErrorResponse {\n  return 'error' in response;\n}\n\n/** Return true if the message is a function call message. */\nexport function isFunctionCallMessage(message: ChatCompletionsDelta): message is FunctionCallMessage {\n  return 'name' in message && 'arguments' in message;\n}\n\n/** Return true if the message is a tool calls message. */\nexport function isToolCallsMessage(message: ChatCompletionsDelta): message is ToolCallsMessage {\n  return 'tool_calls' in message && message.tool_calls != null;\n}\n\n/**\n * An rxjs operator that extracts the content messages from a stream of chat completion responses.\n *\n * @returns An observable that emits the content messages. Each emission will be a string containing the\n *         token emitted by the model.\n * @example <caption>Example of reading all tokens in a stream.</caption>\n * const stream = streamChatCompletions({ model: Model.BASE, messages: [\n *   { role: 'system', content: 'You are a great bot.' },\n *   { role: 'user', content: 'Hello, bot.' },\n * ]}).pipe(extractContent());\n * stream.subscribe({ next: console.log, error: console.error });\n * // Output:\n * // ['Hello', '? ', 'How ', 'are ', 'you', '?']\n */\nexport function extractContent(): UnaryFunction<\n  Observable<ChatCompletionsResponse<ChatCompletionsChunk>>,\n  Observable<string>\n> {\n  return pipe(\n    filter((response: ChatCompletionsResponse<ChatCompletionsChunk>) => isContentMessage(response.choices[0].delta)),\n    // The type assertion is needed here because the type predicate above doesn't seem to propagate.\n    map(\n      (response: ChatCompletionsResponse<ChatCompletionsChunk>) => (response.choices[0].delta as ContentMessage).content\n    )\n  );\n}\n\n/**\n * An rxjs operator that accumulates the content messages from a stream of chat completion responses.\n *\n * @returns An observable that emits the accumulated content messages. Each emission will be a string containing the\n *         content of all messages received so far.\n * @example\n * const stream = streamChatCompletions({ model: Model.BASE, messages: [\n *   { role: 'system', content: 'You are a great bot.' },\n *   { role: 'user', content: 'Hello, bot.' },\n * ]}).pipe(accumulateContent());\n * stream.subscribe({ next: console.log, error: console.error });\n * // Output:\n * // ['Hello', 'Hello! ', 'Hello! How ', 'Hello! How are ', 'Hello! How are you', 'Hello! How are you?']\n */\nexport function accumulateContent(): UnaryFunction<\n  Observable<ChatCompletionsResponse<ChatCompletionsChunk>>,\n  Observable<string>\n> {\n  return pipe(\n    extractContent(),\n    scan((acc, curr) => acc + curr, '')\n  );\n}\n\n/**\n * Make a request to the chat-completions API via the Grafana LLM plugin proxy.\n */\nexport async function chatCompletions(request: ChatCompletionsRequest): Promise<ChatCompletionsResponse> {\n  const response = await getBackendSrv().post<ChatCompletionsResponse>(\n    `/api/plugins/grafana-llm-app/resources/${LLM_CHAT_COMPLETIONS_PATH}`,\n    request,\n    {\n      headers: { 'Content-Type': 'application/json' },\n    }\n  );\n  return response;\n}\n\n/**\n * Make a streaming request to the chat-completions API via the Grafana LLM plugin proxy.\n *\n * A stream of tokens will be returned as an `Observable<string>`. Use the `extractContent` operator to\n * filter the stream to only content messages, or the `accumulateContent` operator to obtain a stream of\n * accumulated content messages.\n *\n * The 'done' message will not be emitted; the stream will simply end when this message is encountered.\n *\n * @example <caption>Example of reading all tokens in a stream.</caption>\n * const stream = streamChatCompletions({ model: Model.BASE, messages: [\n *   { role: 'system', content: 'You are a great bot.' },\n *   { role: 'user', content: 'Hello, bot.' },\n * ]}).pipe(extractContent());\n * stream.subscribe({ next: console.log, error: console.error });\n * // Output:\n * // ['Hello', '? ', 'How ', 'are ', 'you', '?']\n *\n * @example <caption>Example of accumulating tokens in a stream.</caption>\n * const stream = streamChatCompletions({ model: Model.BASE, messages: [\n *   { role: 'system', content: 'You are a great bot.' },\n *   { role: 'user', content: 'Hello, bot.' },\n * ]}).pipe(accumulateContent());\n * stream.subscribe({ next: console.log, error: console.error });\n * // Output:\n * // ['Hello', 'Hello! ', 'Hello! How ', 'Hello! How are ', 'Hello! How are you', 'Hello! How are you?']\n */\nexport function streamChatCompletions(\n  request: ChatCompletionsRequest\n): Observable<ChatCompletionsResponse<ChatCompletionsChunk>> {\n  const channel: LiveChannelAddress = {\n    scope: LiveChannelScope.Plugin,\n    namespace: LLM_PLUGIN_ID,\n    path: LLM_CHAT_COMPLETIONS_PATH + '/' + uuidv4(),\n    data: request,\n  };\n  const messages = getGrafanaLiveSrv()\n    .getStream(channel)\n    .pipe(filter((event) => isLiveChannelMessageEvent(event))) as Observable<\n      LiveChannelMessageEvent<ChatCompletionsResponse<ChatCompletionsChunk>>\n    >;\n  return messages.pipe(\n    // Filter out messages that don't have the expected structure\n    filter((event) => {\n      // Skip messages with null choices\n      if (!event.message.choices) {\n        return false;\n      }\n      return true;\n    }),\n    tap((event) => {\n      if (isErrorResponse(event.message)) {\n        throw new Error(event.message.error);\n      }\n    }),\n    // Stop the stream when we get a done message or when the finish_reason is \"stop\"\n    takeWhile((event) => {\n      // If it's an error response, we should continue to let the tap operator handle it\n      if (isErrorResponse(event.message)) {\n        return true;\n      }\n      \n      // Check for the explicit done message\n      if (event.message.choices && \n          event.message.choices[0].delta && \n          'done' in event.message.choices[0].delta && \n          event.message.choices[0].delta.done === true) {\n        return false;\n      }\n      \n      // Check for finish_reason = \"stop\"\n      if (event.message.choices && \n          'finish_reason' in event.message.choices[0] &&\n          event.message.choices[0].finish_reason === \"stop\") {\n        return false;\n      }\n      \n      return true;\n    }),\n    map((event) => event.message),\n  );\n}\n\nlet loggedWarning = false;\n\n/** Check if the LLM provider API is enabled via the LLM plugin. */\nexport const health = async (): Promise<LLMProviderHealthDetails> => {\n  // First check if the plugin is enabled.\n  try {\n    const settings = await getBackendSrv().get(`${LLM_PLUGIN_ROUTE}/settings`, undefined, undefined, {\n      showSuccessAlert: false,\n      showErrorAlert: false,\n    });\n    if (!settings.enabled) {\n      return { configured: false, ok: false, error: 'The Grafana LLM plugin is not enabled.' };\n    }\n  } catch (e) {\n    logDebug(String(e));\n    logDebug(\n      'Failed to check if LLM provider is enabled. This is expected if the Grafana LLM plugin is not installed, and the above error can be ignored.'\n    );\n    loggedWarning = true;\n    return { configured: false, ok: false, error: 'The Grafana LLM plugin is not installed.' };\n  }\n\n  // Run a health check to see if the LLM provider is configured on the plugin.\n  let response: HealthCheckResponse;\n  try {\n    response = await getBackendSrv().get(`${LLM_PLUGIN_ROUTE}/health`, undefined, undefined, {\n      showSuccessAlert: false,\n      showErrorAlert: false,\n    });\n  } catch (e) {\n    if (!loggedWarning) {\n      logDebug(String(e));\n      logDebug(\n        'Failed to check if LLM provider is enabled. This is expected if the Grafana LLM plugin is not installed, and the above error can be ignored.'\n      );\n      loggedWarning = true;\n    }\n    return { configured: false, ok: false, error: 'The Grafana LLM plugin is not installed.' };\n  }\n\n  const { details } = response;\n  // Update the version if it's present on the response.\n  if (details?.version !== undefined) {\n    setLLMPluginVersion(details.version);\n  }\n  if (details?.llmProvider === undefined) {\n    return { configured: false, ok: false, error: 'The Grafana LLM plugin is outdated; please update it.' };\n  }\n  return typeof details.llmProvider === 'boolean' ? { configured: details.llmProvider, ok: details.llmProvider } : details.llmProvider;\n};\n\nexport const enabled = async (): Promise<boolean> => {\n  const healthDetails = await health();\n  return healthDetails.configured && healthDetails.ok;\n};\n\n/**\n * Enum representing different states for a stream.\n * @enum {string}\n */\nexport enum StreamStatus {\n  IDLE = 'idle',\n  GENERATING = 'generating',\n  COMPLETED = 'completed',\n}\n\n/**\n * A constant representing the timeout value in milliseconds.\n * @type {number}\n */\nexport const TIMEOUT = 10000;\n\n/**\n * A type representing the state of an LLM stream.\n * @typedef {Object} LLMStreamState\n * @property {React.Dispatch<React.SetStateAction<Message[]>} setMessages - A function to set messages.\n * @property {string} reply - The reply associated with the stream.\n * @property {typeof StreamStatus} streamStatus - The current status of the stream.\n * @property {Error|undefined} error - An optional error associated with the stream.\n * @property {{\n *    enabled: boolean|undefined;\n *    stream?: undefined;\n *  }|{\n *    enabled: boolean|undefined;\n *    stream: Subscription;\n *  }|undefined} value - A value that can be an object with 'enabled' and 'stream' properties or undefined.\n */\nexport type LLMStreamState = {\n  setMessages: React.Dispatch<React.SetStateAction<Message[]>>;\n  reply: string;\n  streamStatus: StreamStatus;\n  error: Error | undefined;\n  value:\n  | {\n    enabled: boolean | undefined;\n    stream?: undefined;\n  }\n  | {\n    enabled: boolean | undefined;\n    stream: Subscription;\n  }\n  | undefined;\n};\n\n/**\n * A custom React hook for managing an LLM stream that communicates with the provided model.\n *\n * @param {string} [model=Model.LARGE] - The LLM model to use for communication.\n * @param {number} [temperature=1] - The temperature value for text generation (default is 1).\n * @param {function} [notifyError] - A callback function for handling errors.\n *\n * @returns {LLMStreamState} - An object containing the state of the LLM stream.\n * @property {function} setMessages - A function to update the list of messages in the stream.\n * @property {string} reply - The most recent reply received from the LLM stream.\n * @property {StreamStatus} streamStatus - The status of the stream (\"idle\", \"generating\" or \"completed\").\n * @property {Error|undefined} error - An error object if an error occurs, or undefined if no error.\n * @property {object|undefined} value - The current value of the stream.\n * @property {boolean|undefined} value.enabled - Indicates whether the stream is enabled (true or false).\n * @property {Subscription|undefined} value.stream - The stream subscription object if the stream is active, or undefined if not.\n */\nexport function useLLMStream(\n  model = Model.LARGE,\n  temperature = 1,\n  notifyError: (title: string, text?: string, traceId?: string) => void = () => {}\n): LLMStreamState {\n  // The messages array to send to the LLM.\n  const [messages, setMessages] = useState<Message[]>([]);\n  // The latest reply from the LLM.\n  const [reply, setReply] = useState('');\n  const [streamStatus, setStreamStatus] = useState<StreamStatus>(StreamStatus.IDLE);\n  const [error, setError] = useState<Error>();\n\n  const onError = useCallback(\n    (e: Error) => {\n      setStreamStatus(StreamStatus.IDLE);\n      setMessages([]);\n      setError(e);\n      notifyError(\n        'Failed to generate content using LLM provider',\n        `Please try again or if the problem persists, contact your organization admin.`\n      );\n      console.error(e);\n    },\n    [notifyError]\n  );\n\n  const { error: enabledError, value: isEnabled } = useAsync(async () => await enabled(), [enabled]);\n\n  const { error: asyncError, value } = useAsync(async () => {\n    if (!isEnabled || !messages.length) {\n      return { enabled: isEnabled };\n    }\n\n    setStreamStatus(StreamStatus.GENERATING);\n    setError(undefined);\n    // Stream the completions. Each element is the next stream chunk.\n    const stream = streamChatCompletions({\n      model,\n      temperature,\n      messages,\n    }).pipe(\n      // Accumulate the stream content into a stream of strings, where each\n      // element contains the accumulated message so far.\n      accumulateContent()\n      // The stream is just a regular Observable, so we can use standard rxjs\n      // functionality to update state, e.g. recording when the stream\n      // has completed.\n      // The operator decision tree on the rxjs website is a useful resource:\n      // https://rxjs.dev/operator-decision-tree.)\n    );\n    // Subscribe to the stream and update the state for each returned value.\n    return {\n      enabled: isEnabled,\n      stream: stream.subscribe({\n        next: setReply,\n        error: onError,\n        complete: () => {\n          setStreamStatus(StreamStatus.COMPLETED);\n          setTimeout(() => {\n            setStreamStatus(StreamStatus.IDLE);\n          });\n          setMessages([]);\n          setError(undefined);\n        },\n      }),\n    };\n  }, [messages, isEnabled]);\n\n  // Unsubscribe from the stream when the component unmounts.\n  useEffect(() => {\n    return () => {\n      if (value?.stream) {\n        value.stream.unsubscribe();\n      }\n    };\n  }, [value]);\n\n  // If the stream is generating and we haven't received a reply, it times out.\n  useEffect(() => {\n    let timeout: NodeJS.Timeout | undefined;\n    if (streamStatus === StreamStatus.GENERATING && reply === '') {\n      timeout = setTimeout(() => {\n        onError(new Error(`LLM stream timed out after ${TIMEOUT}ms`));\n      }, TIMEOUT);\n    }\n    return () => {\n      timeout && clearTimeout(timeout);\n    };\n  }, [streamStatus, reply, onError]);\n\n  if (asyncError || enabledError) {\n    setError(asyncError || enabledError);\n  }\n\n  return {\n    setMessages,\n    reply,\n    streamStatus,\n    error,\n    value,\n  };\n}\n\n/**\n * An rxjs operator that accumulates tool call messages from a stream of chat completion responses into a complete tool call message.\n * \n * @returns An observable that emits the accumulated tool call message when complete.\n * @example\n * const stream = streamChatCompletions({...}).pipe(\n *   accumulateToolCalls()\n * );\n * stream.subscribe({\n *   next: (toolCallMessage) => console.log('Received complete tool call:', toolCallMessage),\n *   error: console.error\n * });\n */\nexport function accumulateToolCalls(): UnaryFunction<\n  Observable<ChatCompletionsResponse<ChatCompletionsChunk>>,\n  Observable<ToolCallsMessage>\n> {\n  return pipe(\n    filter((response: ChatCompletionsResponse<ChatCompletionsChunk>) => isToolCallsMessage(response.choices[0].delta)),\n    // Collect all tool call chunks\n    toArray(),\n    // Process the array to reconstruct the complete tool call message\n    map((responses: Array<ChatCompletionsResponse<ChatCompletionsChunk>>) => {\n      const toolCallChunks = responses.map(r => r.choices[0].delta as ToolCallsMessage);\n      return recoverToolCallMessage(toolCallChunks);\n    })\n  );\n}\n\n/**\n * Recovers a complete tool call message from individual chunks.\n * \n * @param toolCallMessages - Array of tool call message chunks\n * @returns A complete tool call message with all chunks combined\n */\nexport function recoverToolCallMessage(toolCallMessages: ToolCallsMessage[]): ToolCallsMessage {\n  const recoveredToolCallMessage: ToolCallsMessage = {\n    role: 'assistant',\n    tool_calls: [],\n  };\n\n  for (const msg of toolCallMessages) {\n    for (const tc of msg.tool_calls) {\n      if (tc.index! >= recoveredToolCallMessage.tool_calls.length) {\n        recoveredToolCallMessage.tool_calls.push({ \n          ...tc, \n          function: { ...tc.function, arguments: tc.function.arguments ?? '' } \n        });\n      } else {\n        recoveredToolCallMessage.tool_calls[tc.index!].function.arguments += tc.function.arguments ?? '';\n      }\n    }\n  }\n\n  // Ensure final arguments are never empty\n  for (const tc of recoveredToolCallMessage.tool_calls) {\n    if (!tc.function.arguments) {\n      tc.function.arguments = '{}';\n    }\n  }\n\n  return recoveredToolCallMessage;\n}\n"],"names":["Model","uuidv4","StreamStatus"],"mappings":";;;;;;;;;AA2BA,MAAM,yBAA4B,GAAA,yBAAA;AAgGtB,IAAA,KAAA,qBAAAA,MAAL,KAAA;AACL,EAAAA,OAAA,MAAO,CAAA,GAAA,MAAA;AACP,EAAAA,OAAA,OAAQ,CAAA,GAAA,OAAA;AAFE,EAAAA,OAAAA,MAAAA;AAAA,CAAA,EAAA,KAAA,IAAA,EAAA;AAmLL,SAAS,iBAAiB,OAA0D,EAAA;AACzF,EAAA,OAAO,SAAa,IAAA,OAAA;AACtB;AAGO,SAAS,cAAc,OAAuD,EAAA;AACnF,EAAO,OAAA,MAAA,IAAU,OAAW,IAAA,OAAA,CAAQ,IAAQ,IAAA,IAAA;AAC9C;AAGO,SAAS,gBACd,QAC0C,EAAA;AAC1C,EAAA,OAAO,OAAW,IAAA,QAAA;AACpB;AAGO,SAAS,sBAAsB,OAA+D,EAAA;AACnG,EAAO,OAAA,MAAA,IAAU,WAAW,WAAe,IAAA,OAAA;AAC7C;AAGO,SAAS,mBAAmB,OAA4D,EAAA;AAC7F,EAAO,OAAA,YAAA,IAAgB,OAAW,IAAA,OAAA,CAAQ,UAAc,IAAA,IAAA;AAC1D;AAgBO,SAAS,cAGd,GAAA;AACA,EAAO,OAAA,IAAA;AAAA,IACL,MAAA,CAAO,CAAC,QAA4D,KAAA,gBAAA,CAAiB,SAAS,OAAQ,CAAA,CAAC,CAAE,CAAA,KAAK,CAAC,CAAA;AAAA;AAAA,IAE/G,GAAA;AAAA,MACE,CAAC,QAA6D,KAAA,QAAA,CAAS,OAAQ,CAAA,CAAC,EAAE,KAAyB,CAAA;AAAA;AAC7G,GACF;AACF;AAgBO,SAAS,iBAGd,GAAA;AACA,EAAO,OAAA,IAAA;AAAA,IACL,cAAe,EAAA;AAAA,IACf,KAAK,CAAC,GAAA,EAAK,IAAS,KAAA,GAAA,GAAM,MAAM,EAAE;AAAA,GACpC;AACF;AAKA,eAAsB,gBAAgB,OAAmE,EAAA;AACvG,EAAM,MAAA,QAAA,GAAW,MAAM,aAAA,EAAgB,CAAA,IAAA;AAAA,IACrC,0CAA0C,yBAAyB,CAAA,CAAA;AAAA,IACnE,OAAA;AAAA,IACA;AAAA,MACE,OAAA,EAAS,EAAE,cAAA,EAAgB,kBAAmB;AAAA;AAChD,GACF;AACA,EAAO,OAAA,QAAA;AACT;AA6BO,SAAS,sBACd,OAC2D,EAAA;AAC3D,EAAA,MAAM,OAA8B,GAAA;AAAA,IAClC,OAAO,gBAAiB,CAAA,MAAA;AAAA,IACxB,SAAW,EAAA,aAAA;AAAA,IACX,IAAA,EAAM,yBAA4B,GAAA,GAAA,GAAMC,EAAO,EAAA;AAAA,IAC/C,IAAM,EAAA;AAAA,GACR;AACA,EAAA,MAAM,QAAW,GAAA,iBAAA,EACd,CAAA,SAAA,CAAU,OAAO,CAAA,CACjB,IAAK,CAAA,MAAA,CAAO,CAAC,KAAA,KAAU,yBAA0B,CAAA,KAAK,CAAC,CAAC,CAAA;AAG3D,EAAA,OAAO,QAAS,CAAA,IAAA;AAAA;AAAA,IAEd,MAAA,CAAO,CAAC,KAAU,KAAA;AAEhB,MAAI,IAAA,CAAC,KAAM,CAAA,OAAA,CAAQ,OAAS,EAAA;AAC1B,QAAO,OAAA,KAAA;AAAA;AAET,MAAO,OAAA,IAAA;AAAA,KACR,CAAA;AAAA,IACD,GAAA,CAAI,CAAC,KAAU,KAAA;AACb,MAAI,IAAA,eAAA,CAAgB,KAAM,CAAA,OAAO,CAAG,EAAA;AAClC,QAAA,MAAM,IAAI,KAAA,CAAM,KAAM,CAAA,OAAA,CAAQ,KAAK,CAAA;AAAA;AACrC,KACD,CAAA;AAAA;AAAA,IAED,SAAA,CAAU,CAAC,KAAU,KAAA;AAEnB,MAAI,IAAA,eAAA,CAAgB,KAAM,CAAA,OAAO,CAAG,EAAA;AAClC,QAAO,OAAA,IAAA;AAAA;AAIT,MAAI,IAAA,KAAA,CAAM,QAAQ,OACd,IAAA,KAAA,CAAM,QAAQ,OAAQ,CAAA,CAAC,CAAE,CAAA,KAAA,IACzB,MAAU,IAAA,KAAA,CAAM,QAAQ,OAAQ,CAAA,CAAC,CAAE,CAAA,KAAA,IACnC,KAAM,CAAA,OAAA,CAAQ,QAAQ,CAAC,CAAA,CAAE,KAAM,CAAA,IAAA,KAAS,IAAM,EAAA;AAChD,QAAO,OAAA,KAAA;AAAA;AAIT,MAAA,IAAI,KAAM,CAAA,OAAA,CAAQ,OACd,IAAA,eAAA,IAAmB,MAAM,OAAQ,CAAA,OAAA,CAAQ,CAAC,CAAA,IAC1C,MAAM,OAAQ,CAAA,OAAA,CAAQ,CAAC,CAAA,CAAE,kBAAkB,MAAQ,EAAA;AACrD,QAAO,OAAA,KAAA;AAAA;AAGT,MAAO,OAAA,IAAA;AAAA,KACR,CAAA;AAAA,IACD,GAAI,CAAA,CAAC,KAAU,KAAA,KAAA,CAAM,OAAO;AAAA,GAC9B;AACF;AAEA,IAAI,aAAgB,GAAA,KAAA;AAGb,MAAM,SAAS,YAA+C;AAEnE,EAAI,IAAA;AACF,IAAM,MAAA,QAAA,GAAW,MAAM,aAAc,EAAA,CAAE,IAAI,CAAG,EAAA,gBAAgB,CAAa,SAAA,CAAA,EAAA,KAAA,CAAA,EAAW,KAAW,CAAA,EAAA;AAAA,MAC/F,gBAAkB,EAAA,KAAA;AAAA,MAClB,cAAgB,EAAA;AAAA,KACjB,CAAA;AACD,IAAI,IAAA,CAAC,SAAS,OAAS,EAAA;AACrB,MAAA,OAAO,EAAE,UAAY,EAAA,KAAA,EAAO,EAAI,EAAA,KAAA,EAAO,OAAO,wCAAyC,EAAA;AAAA;AACzF,WACO,CAAG,EAAA;AACV,IAAS,QAAA,CAAA,MAAA,CAAO,CAAC,CAAC,CAAA;AAClB,IAAA,QAAA;AAAA,MACE;AAAA,KACF;AACA,IAAgB,aAAA,GAAA,IAAA;AAChB,IAAA,OAAO,EAAE,UAAY,EAAA,KAAA,EAAO,EAAI,EAAA,KAAA,EAAO,OAAO,0CAA2C,EAAA;AAAA;AAI3F,EAAI,IAAA,QAAA;AACJ,EAAI,IAAA;AACF,IAAW,QAAA,GAAA,MAAM,eAAgB,CAAA,GAAA,CAAI,GAAG,gBAAgB,CAAA,OAAA,CAAA,EAAW,QAAW,KAAW,CAAA,EAAA;AAAA,MACvF,gBAAkB,EAAA,KAAA;AAAA,MAClB,cAAgB,EAAA;AAAA,KACjB,CAAA;AAAA,WACM,CAAG,EAAA;AACV,IAAA,IAAI,CAAC,aAAe,EAAA;AAClB,MAAS,QAAA,CAAA,MAAA,CAAO,CAAC,CAAC,CAAA;AAClB,MAAA,QAAA;AAAA,QACE;AAAA,OACF;AACA,MAAgB,aAAA,GAAA,IAAA;AAAA;AAElB,IAAA,OAAO,EAAE,UAAY,EAAA,KAAA,EAAO,EAAI,EAAA,KAAA,EAAO,OAAO,0CAA2C,EAAA;AAAA;AAG3F,EAAM,MAAA,EAAE,SAAY,GAAA,QAAA;AAEpB,EAAI,IAAA,OAAA,EAAS,YAAY,SAAW,EAAA;AAClC,IAAA,mBAAA,CAAoB,QAAQ,OAAO,CAAA;AAAA;AAErC,EAAI,IAAA,OAAA,EAAS,gBAAgB,SAAW,EAAA;AACtC,IAAA,OAAO,EAAE,UAAY,EAAA,KAAA,EAAO,EAAI,EAAA,KAAA,EAAO,OAAO,uDAAwD,EAAA;AAAA;AAExG,EAAA,OAAO,OAAO,OAAA,CAAQ,WAAgB,KAAA,SAAA,GAAY,EAAE,UAAA,EAAY,OAAQ,CAAA,WAAA,EAAa,EAAI,EAAA,OAAA,CAAQ,WAAY,EAAA,GAAI,OAAQ,CAAA,WAAA;AAC3H;AAEO,MAAM,UAAU,YAA8B;AACnD,EAAM,MAAA,aAAA,GAAgB,MAAM,MAAO,EAAA;AACnC,EAAO,OAAA,aAAA,CAAc,cAAc,aAAc,CAAA,EAAA;AACnD;AAMY,IAAA,YAAA,qBAAAC,aAAL,KAAA;AACL,EAAAA,cAAA,MAAO,CAAA,GAAA,MAAA;AACP,EAAAA,cAAA,YAAa,CAAA,GAAA,YAAA;AACb,EAAAA,cAAA,WAAY,CAAA,GAAA,WAAA;AAHF,EAAAA,OAAAA,aAAAA;AAAA,CAAA,EAAA,YAAA,IAAA,EAAA;AAUL,MAAM,OAAU,GAAA;AAkDhB,SAAS,aACd,KAAQ,GAAA,OAAA,cACR,WAAc,GAAA,CAAA,EACd,cAAwE,MAAM;AAAC,CAC/D,EAAA;AAEhB,EAAA,MAAM,CAAC,QAAU,EAAA,WAAW,CAAI,GAAA,QAAA,CAAoB,EAAE,CAAA;AAEtD,EAAA,MAAM,CAAC,KAAA,EAAO,QAAQ,CAAA,GAAI,SAAS,EAAE,CAAA;AACrC,EAAA,MAAM,CAAC,YAAA,EAAc,eAAe,CAAA,GAAI,SAAuB,MAAiB,YAAA;AAChF,EAAA,MAAM,CAAC,KAAA,EAAO,QAAQ,CAAA,GAAI,QAAgB,EAAA;AAE1C,EAAA,MAAM,OAAU,GAAA,WAAA;AAAA,IACd,CAAC,CAAa,KAAA;AACZ,MAAA,eAAA,CAAgB,MAAiB,YAAA;AACjC,MAAA,WAAA,CAAY,EAAE,CAAA;AACd,MAAA,QAAA,CAAS,CAAC,CAAA;AACV,MAAA,WAAA;AAAA,QACE,+CAAA;AAAA,QACA,CAAA,6EAAA;AAAA,OACF;AACA,MAAA,OAAA,CAAQ,MAAM,CAAC,CAAA;AAAA,KACjB;AAAA,IACA,CAAC,WAAW;AAAA,GACd;AAEA,EAAA,MAAM,EAAE,KAAA,EAAO,YAAc,EAAA,KAAA,EAAO,SAAU,EAAA,GAAI,QAAS,CAAA,YAAY,MAAM,OAAA,EAAW,EAAA,CAAC,OAAO,CAAC,CAAA;AAEjG,EAAA,MAAM,EAAE,KAAO,EAAA,UAAA,EAAY,KAAM,EAAA,GAAI,SAAS,YAAY;AACxD,IAAA,IAAI,CAAC,SAAA,IAAa,CAAC,QAAA,CAAS,MAAQ,EAAA;AAClC,MAAO,OAAA,EAAE,SAAS,SAAU,EAAA;AAAA;AAG9B,IAAA,eAAA,CAAgB,YAAuB,kBAAA;AACvC,IAAA,QAAA,CAAS,SAAS,CAAA;AAElB,IAAA,MAAM,SAAS,qBAAsB,CAAA;AAAA,MACnC,KAAA;AAAA,MACA,WAAA;AAAA,MACA;AAAA,KACD,CAAE,CAAA,IAAA;AAAA;AAAA;AAAA,MAGD,iBAAkB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,KAMpB;AAEA,IAAO,OAAA;AAAA,MACL,OAAS,EAAA,SAAA;AAAA,MACT,MAAA,EAAQ,OAAO,SAAU,CAAA;AAAA,QACvB,IAAM,EAAA,QAAA;AAAA,QACN,KAAO,EAAA,OAAA;AAAA,QACP,UAAU,MAAM;AACd,UAAA,eAAA,CAAgB,WAAsB,iBAAA;AACtC,UAAA,UAAA,CAAW,MAAM;AACf,YAAA,eAAA,CAAgB,MAAiB,YAAA;AAAA,WAClC,CAAA;AACD,UAAA,WAAA,CAAY,EAAE,CAAA;AACd,UAAA,QAAA,CAAS,SAAS,CAAA;AAAA;AACpB,OACD;AAAA,KACH;AAAA,GACC,EAAA,CAAC,QAAU,EAAA,SAAS,CAAC,CAAA;AAGxB,EAAA,SAAA,CAAU,MAAM;AACd,IAAA,OAAO,MAAM;AACX,MAAA,IAAI,OAAO,MAAQ,EAAA;AACjB,QAAA,KAAA,CAAM,OAAO,WAAY,EAAA;AAAA;AAC3B,KACF;AAAA,GACF,EAAG,CAAC,KAAK,CAAC,CAAA;AAGV,EAAA,SAAA,CAAU,MAAM;AACd,IAAI,IAAA,OAAA;AACJ,IAAI,IAAA,YAAA,KAAiB,YAA2B,qBAAA,KAAA,KAAU,EAAI,EAAA;AAC5D,MAAA,OAAA,GAAU,WAAW,MAAM;AACzB,QAAA,OAAA,CAAQ,IAAI,KAAA,CAAM,CAA8B,2BAAA,EAAA,OAAO,IAAI,CAAC,CAAA;AAAA,SAC3D,OAAO,CAAA;AAAA;AAEZ,IAAA,OAAO,MAAM;AACX,MAAA,OAAA,IAAW,aAAa,OAAO,CAAA;AAAA,KACjC;AAAA,GACC,EAAA,CAAC,YAAc,EAAA,KAAA,EAAO,OAAO,CAAC,CAAA;AAEjC,EAAA,IAAI,cAAc,YAAc,EAAA;AAC9B,IAAA,QAAA,CAAS,cAAc,YAAY,CAAA;AAAA;AAGrC,EAAO,OAAA;AAAA,IACL,WAAA;AAAA,IACA,KAAA;AAAA,IACA,YAAA;AAAA,IACA,KAAA;AAAA,IACA;AAAA,GACF;AACF;AAeO,SAAS,mBAGd,GAAA;AACA,EAAO,OAAA,IAAA;AAAA,IACL,MAAA,CAAO,CAAC,QAA4D,KAAA,kBAAA,CAAmB,SAAS,OAAQ,CAAA,CAAC,CAAE,CAAA,KAAK,CAAC,CAAA;AAAA;AAAA,IAEjH,OAAQ,EAAA;AAAA;AAAA,IAER,GAAA,CAAI,CAAC,SAAoE,KAAA;AACvE,MAAM,MAAA,cAAA,GAAiB,UAAU,GAAI,CAAA,CAAA,CAAA,KAAK,EAAE,OAAQ,CAAA,CAAC,EAAE,KAAyB,CAAA;AAChF,MAAA,OAAO,uBAAuB,cAAc,CAAA;AAAA,KAC7C;AAAA,GACH;AACF;AAQO,SAAS,uBAAuB,gBAAwD,EAAA;AAC7F,EAAA,MAAM,wBAA6C,GAAA;AAAA,IACjD,IAAM,EAAA,WAAA;AAAA,IACN,YAAY;AAAC,GACf;AAEA,EAAA,KAAA,MAAW,OAAO,gBAAkB,EAAA;AAClC,IAAW,KAAA,MAAA,EAAA,IAAM,IAAI,UAAY,EAAA;AAC/B,MAAA,IAAI,EAAG,CAAA,KAAA,IAAU,wBAAyB,CAAA,UAAA,CAAW,MAAQ,EAAA;AAC3D,QAAA,wBAAA,CAAyB,WAAW,IAAK,CAAA;AAAA,UACvC,GAAG,EAAA;AAAA,UACH,QAAA,EAAU,EAAE,GAAG,EAAA,CAAG,UAAU,SAAW,EAAA,EAAA,CAAG,QAAS,CAAA,SAAA,IAAa,EAAG;AAAA,SACpE,CAAA;AAAA,OACI,MAAA;AACL,QAAyB,wBAAA,CAAA,UAAA,CAAW,GAAG,KAAM,CAAA,CAAE,SAAS,SAAa,IAAA,EAAA,CAAG,SAAS,SAAa,IAAA,EAAA;AAAA;AAChG;AACF;AAIF,EAAW,KAAA,MAAA,EAAA,IAAM,yBAAyB,UAAY,EAAA;AACpD,IAAI,IAAA,CAAC,EAAG,CAAA,QAAA,CAAS,SAAW,EAAA;AAC1B,MAAA,EAAA,CAAG,SAAS,SAAY,GAAA,IAAA;AAAA;AAC1B;AAGF,EAAO,OAAA,wBAAA;AACT;;;;"}