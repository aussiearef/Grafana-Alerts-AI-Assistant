import { getBackendSrv } from '@grafana/runtime';
import { LLM_PLUGIN_ROUTE } from './constants.js';
export { Model, StreamStatus, TIMEOUT, accumulateContent, accumulateToolCalls, chatCompletions, extractContent, health, isContentMessage, isDoneMessage, isErrorResponse, isFunctionCallMessage, isToolCallsMessage, recoverToolCallMessage, streamChatCompletions, useLLMStream } from './llm.js';

const enabled = async () => {
  try {
    const settings = await getBackendSrv().get(`${LLM_PLUGIN_ROUTE}/settings`);
    if (!settings.enabled) {
      return false;
    }
    const health = await getBackendSrv().get(`${LLM_PLUGIN_ROUTE}/health`);
    const details = health.details;
    if (details.llmProvider) {
      return details.llmProvider.configured && details.llmProvider.ok;
    }
    if (details.openAI) {
      return details.openAI.configured && details.openAI.ok;
    }
    return false;
  } catch (e) {
    return false;
  }
};

export { enabled };
//# sourceMappingURL=openai.js.map
