{"version":3,"file":"index.js","sources":["../src/constants.ts","../src/llm.ts","../src/openai.ts","../src/mcp.tsx","../src/vector.ts"],"sourcesContent":["import { logWarning } from '@grafana/runtime';\n\nimport { SemVer } from 'semver';\n\nexport const LLM_PLUGIN_ID = 'grafana-llm-app';\nexport const LLM_PLUGIN_ROUTE = `/api/plugins/${LLM_PLUGIN_ID}`;\n\n// The LLM app was at version 0.2.0 before we added the health check.\n// If the health check fails, or the details don't exist on the response,\n// we should assume it's this older version.\nexport let LLM_PLUGIN_VERSION = new SemVer('0.2.0');\n\nexport function setLLMPluginVersion(version: string) {\n  try {\n    LLM_PLUGIN_VERSION = new SemVer(version);\n  } catch (e) {\n    logWarning('Failed to parse version of grafana-llm-app; assuming old version is present.');\n  }\n}\n","/**\n * LLM API client.\n *\n * This module contains functions used to make requests to the LLM provider API via\n * the Grafana LLM app plugin. That plugin must be installed, enabled and configured\n * in order for these functions to work.\n *\n * The {@link enabled} function can be used to check if the plugin is enabled and configured.\n */\n\nimport {\n  isLiveChannelMessageEvent,\n  LiveChannelAddress,\n  LiveChannelMessageEvent,\n  LiveChannelScope,\n} from '@grafana/data';\nimport { getBackendSrv, getGrafanaLiveSrv, logDebug /* logError */ } from '@grafana/runtime';\n\nimport React, { useEffect, useCallback, useState } from 'react';\nimport { useAsync } from 'react-use';\nimport { pipe, Observable, UnaryFunction, Subscription } from 'rxjs';\nimport { filter, map, scan, takeWhile, tap, toArray } from 'rxjs/operators';\nimport { v4 as uuidv4 } from 'uuid';\n\nimport { LLM_PLUGIN_ID, LLM_PLUGIN_ROUTE, setLLMPluginVersion } from './constants';\nimport { HealthCheckResponse, LLMProviderHealthDetails } from './types';\n\nconst LLM_CHAT_COMPLETIONS_PATH = 'llm/v1/chat/completions';\n\n/** The role of a message's author. */\nexport type Role = 'system' | 'user' | 'assistant' | 'function' | 'tool';\n\n/** A message in a conversation. */\nexport interface Message {\n  /** The role of the message's author. */\n  role: Role;\n\n  /** The contents of the message. content is required for all messages, and may be null for assistant messages with function calls. */\n  content?: string;\n\n  /** The ID of the tool call, if this message is a function call. */\n  tool_call_id?: string;\n\n  /**\n   * The name of the author of this message.\n   *\n   * This is required if role is 'function', and it should be the name of the function whose response is in the content.\n   *\n   * May contain a-z, A-Z, 0-9, and underscores, with a maximum length of 64 characters.\n   */\n  name?: string;\n\n  /**\n   * The name and arguments of a function that should be called, as generated by the model.\n   *\n   * @deprecated Use tool_calls instead.\n   */\n  function_call?: Object;\n\n  /**\n   * The tool calls generated by the model, such as function calls.\n   */\n  tool_calls?: ToolCall[];\n}\n\n\n/** A tool call the model may generate. */\nexport interface ToolCall {\n  id: string;\n  index?: number;\n  type: 'function';\n  function: FunctionCall;\n}\n\n/** A function call generated by the model. */\ninterface FunctionCall {\n  /**\n   * The name of the tool to call.\n   */\n  name: string;\n\n  /**\n   * The arguments to call the function with, as generated by the model in JSON format.\n   *\n   * Note that the model does not always generate valid JSON, and may hallucinate\n   * parameters not defined by your function schema. Validate the arguments in\n   * your code before calling your function.\n   */\n  arguments: string;\n}\n\n\n/** A function the model may generate JSON inputs for. */\nexport interface Function {\n  /**\n   * The name of the function to be called.\n   *\n   * Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.\n   */\n  name: string;\n  /**\n   * A description of what the function does, used by the model to choose when and how to call the function.\n   */\n  description?: string;\n  /*\n   * The parameters the functions accepts, described as a JSON Schema object. See the provider's guide for examples, and the JSON Schema reference for documentation about the format.\n   *\n   * Omitting `parameters` defines a function with an empty parameter list.\n   */\n  parameters?: Object;\n  /**\n   * Whether to enable strict schema adherence when generating the function call.\n   *\n   * If set to true, the model will follow the exact schema defined in the parameters field.\n   * Only a subset of JSON Schema is supported when strict is true.\n   */\n  strict?: boolean;\n}\n\n/**\n * Enum representing abstracted models used by the backend app.\n * @enum {string}\n */\nexport enum Model {\n  BASE = 'base',\n  LARGE = 'large',\n}\n\n/**\n * @deprecated Use {@link Model} instead.\n */\ntype DeprecatedString = string;\n\nexport interface ChatCompletionsRequest {\n  /**\n   * Model abstraction to use. These abstractions are then translated back into specific models based on the users settings.\n   *\n   * If not specified, defaults to `Model.BASE`.\n   */\n  model?: Model | DeprecatedString;\n  /** A list of messages comprising the conversation so far. */\n  messages: Message[];\n  /**\n   * What sampling temperature to use, between 0 and 2.\n   * Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n   *\n   * We generally recommend altering this or top_p but not both.\n   */\n  temperature?: number;\n  /**\n   * An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.\n   * So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n   *\n   * We generally recommend altering this or temperature but not both.\n   */\n  top_p?: number;\n  /**\n   * How many chat completion choices to generate for each input message.\n   */\n  n?: number;\n  /**\n   * Up to 4 sequences where the API will stop generating further tokens.\n   */\n  stop?: string | string[];\n  /**\n   * The maximum number of tokens to generate in the chat completion.\n   *\n   * The total length of input tokens and generated tokens is limited by the model's context length. Example Python code for counting tokens.\n   */\n  max_tokens?: number;\n  /**\n   * Number between -2.0 and 2.0.\n   *\n   * Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\n   */\n  presence_penalty?: number;\n  /**\n   * Number between -2.0 and 2.0.\n   *\n   * Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\n   */\n  frequency_penalty?: number;\n  /**\n   * Modify the likelihood of specified tokens appearing in the completion.\n   *\n   * Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100.\n   * Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model,\n   * but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban\n   * or exclusive selection of the relevant token.\n   */\n  logit_bias?: { [key: string]: number };\n  /**\n   * A unique identifier representing your end-user, which can help monitor and detect abuse.\n   */\n  user?: string;\n\n  /** A list of tools that the model may use. */\n  tools?: Tool[];\n}\n\n/** A tool that the model may use. */\nexport interface Tool {\n  type: 'function';\n  /** The function that the model may use. */\n  function: Function;\n}\n\n/** A completion object from the LLM provider. */\nexport interface Choice {\n  /** The message object generated by the model. */\n  message: Message;\n  /**\n   * The reason the model stopped generating text.\n   *\n   * This may be one of:\n   *  - stop: API returned complete message, or a message terminated by one of the stop sequences provided via the stop parameter\n   *  - length: incomplete model output due to max_tokens parameter or token limit\n   *  - function_call: the model decided to call a function\n   *  - content_filter: omitted content due to a flag from our content filters\n   *  - null: API response still in progress or incomplete\n   */\n  finish_reason: string;\n  /** The index of the completion in the list of choices. */\n  index: number;\n}\n\n/** The usage statistics for a request to the LLM provider. */\nexport interface Usage {\n  /** The number of tokens in the prompt. */\n  prompt_tokens: number;\n  /** The number of tokens in the completion. */\n  completion_tokens: number;\n  /** The total number of tokens. */\n  total_tokens: number;\n}\n\n/** The error response from the Grafana LLM app when trying to call the chat completions API. */\ninterface ChatCompletionsErrorResponse {\n  /** The error message. */\n  error: string;\n}\n\n/** A response from the LLM provider Chat Completions API. */\nexport interface ChatCompletionsResponse<T = Choice> {\n  /** The ID of the request. */\n  id: string;\n  /** The type of object returned (e.g. 'chat.completion'). */\n  object: string;\n  /** The timestamp of the request, as a UNIX timestamp. */\n  created: number;\n  /** The name of the model used to generate the response. */\n  model: string;\n  /** A list of completion objects (only one, unless `n > 1` in the request). */\n  choices: T[];\n  /** The number of tokens used to generate the replies, counting prompt, completion, and total. */\n  usage: Usage;\n}\n\n/** A content message returned from the model. */\nexport interface ContentMessage {\n  /** The content of the message. */\n  content: string;\n  /** The role of the author of this message. */\n  role: Role;\n}\n\n/** A message returned from the model indicating that it is done. */\nexport interface DoneMessage {\n  done: boolean;\n}\n\n/** A function call message returned from the model. */\nexport interface FunctionCallMessage {\n  /** The name of the function to call. */\n  name: string;\n  /** The arguments to the function call. */\n  arguments: any[];\n}\n\n/** A tool calls message returned from the model. */\nexport interface ToolCallsMessage {\n  /** The tool calls generated by the model. */\n  tool_calls: ToolCall[];\n  /** The role of the author of this message. */\n  role: Role;\n}\n\n/**\n * A delta returned from a stream of chat completion responses.\n *\n * In practice this will be either a content message or a function call;\n * done messages are filtered out by the `streamChatCompletions` function.\n */\nexport type ChatCompletionsDelta = ContentMessage | FunctionCallMessage | DoneMessage | ToolCallsMessage;\n\n/** A chunk included in a chat completion response. */\nexport interface ChatCompletionsChunk {\n  /** The delta since the previous chunk. */\n  delta: ChatCompletionsDelta;\n}\n\n/** Return true if the message is a 'content' message. */\nexport function isContentMessage(message: ChatCompletionsDelta): message is ContentMessage {\n  return 'content' in message;\n}\n\n/** Return true if the message is a 'done' message. */\nexport function isDoneMessage(message: ChatCompletionsDelta): message is DoneMessage {\n  return 'done' in message && message.done != null;\n}\n\n/** Return true if the response is an error response. */\nexport function isErrorResponse<T>(\n  response: ChatCompletionsResponse<T> | ChatCompletionsErrorResponse\n): response is ChatCompletionsErrorResponse {\n  return 'error' in response;\n}\n\n/** Return true if the message is a function call message. */\nexport function isFunctionCallMessage(message: ChatCompletionsDelta): message is FunctionCallMessage {\n  return 'name' in message && 'arguments' in message;\n}\n\n/** Return true if the message is a tool calls message. */\nexport function isToolCallsMessage(message: ChatCompletionsDelta): message is ToolCallsMessage {\n  return 'tool_calls' in message && message.tool_calls != null;\n}\n\n/**\n * An rxjs operator that extracts the content messages from a stream of chat completion responses.\n *\n * @returns An observable that emits the content messages. Each emission will be a string containing the\n *         token emitted by the model.\n * @example <caption>Example of reading all tokens in a stream.</caption>\n * const stream = streamChatCompletions({ model: Model.BASE, messages: [\n *   { role: 'system', content: 'You are a great bot.' },\n *   { role: 'user', content: 'Hello, bot.' },\n * ]}).pipe(extractContent());\n * stream.subscribe({ next: console.log, error: console.error });\n * // Output:\n * // ['Hello', '? ', 'How ', 'are ', 'you', '?']\n */\nexport function extractContent(): UnaryFunction<\n  Observable<ChatCompletionsResponse<ChatCompletionsChunk>>,\n  Observable<string>\n> {\n  return pipe(\n    filter((response: ChatCompletionsResponse<ChatCompletionsChunk>) => isContentMessage(response.choices[0].delta)),\n    // The type assertion is needed here because the type predicate above doesn't seem to propagate.\n    map(\n      (response: ChatCompletionsResponse<ChatCompletionsChunk>) => (response.choices[0].delta as ContentMessage).content\n    )\n  );\n}\n\n/**\n * An rxjs operator that accumulates the content messages from a stream of chat completion responses.\n *\n * @returns An observable that emits the accumulated content messages. Each emission will be a string containing the\n *         content of all messages received so far.\n * @example\n * const stream = streamChatCompletions({ model: Model.BASE, messages: [\n *   { role: 'system', content: 'You are a great bot.' },\n *   { role: 'user', content: 'Hello, bot.' },\n * ]}).pipe(accumulateContent());\n * stream.subscribe({ next: console.log, error: console.error });\n * // Output:\n * // ['Hello', 'Hello! ', 'Hello! How ', 'Hello! How are ', 'Hello! How are you', 'Hello! How are you?']\n */\nexport function accumulateContent(): UnaryFunction<\n  Observable<ChatCompletionsResponse<ChatCompletionsChunk>>,\n  Observable<string>\n> {\n  return pipe(\n    extractContent(),\n    scan((acc, curr) => acc + curr, '')\n  );\n}\n\n/**\n * Make a request to the chat-completions API via the Grafana LLM plugin proxy.\n */\nexport async function chatCompletions(request: ChatCompletionsRequest): Promise<ChatCompletionsResponse> {\n  const response = await getBackendSrv().post<ChatCompletionsResponse>(\n    `/api/plugins/grafana-llm-app/resources/${LLM_CHAT_COMPLETIONS_PATH}`,\n    request,\n    {\n      headers: { 'Content-Type': 'application/json' },\n    }\n  );\n  return response;\n}\n\n/**\n * Make a streaming request to the chat-completions API via the Grafana LLM plugin proxy.\n *\n * A stream of tokens will be returned as an `Observable<string>`. Use the `extractContent` operator to\n * filter the stream to only content messages, or the `accumulateContent` operator to obtain a stream of\n * accumulated content messages.\n *\n * The 'done' message will not be emitted; the stream will simply end when this message is encountered.\n *\n * @example <caption>Example of reading all tokens in a stream.</caption>\n * const stream = streamChatCompletions({ model: Model.BASE, messages: [\n *   { role: 'system', content: 'You are a great bot.' },\n *   { role: 'user', content: 'Hello, bot.' },\n * ]}).pipe(extractContent());\n * stream.subscribe({ next: console.log, error: console.error });\n * // Output:\n * // ['Hello', '? ', 'How ', 'are ', 'you', '?']\n *\n * @example <caption>Example of accumulating tokens in a stream.</caption>\n * const stream = streamChatCompletions({ model: Model.BASE, messages: [\n *   { role: 'system', content: 'You are a great bot.' },\n *   { role: 'user', content: 'Hello, bot.' },\n * ]}).pipe(accumulateContent());\n * stream.subscribe({ next: console.log, error: console.error });\n * // Output:\n * // ['Hello', 'Hello! ', 'Hello! How ', 'Hello! How are ', 'Hello! How are you', 'Hello! How are you?']\n */\nexport function streamChatCompletions(\n  request: ChatCompletionsRequest\n): Observable<ChatCompletionsResponse<ChatCompletionsChunk>> {\n  const channel: LiveChannelAddress = {\n    scope: LiveChannelScope.Plugin,\n    namespace: LLM_PLUGIN_ID,\n    path: LLM_CHAT_COMPLETIONS_PATH + '/' + uuidv4(),\n    data: request,\n  };\n  const messages = getGrafanaLiveSrv()\n    .getStream(channel)\n    .pipe(filter((event) => isLiveChannelMessageEvent(event))) as Observable<\n      LiveChannelMessageEvent<ChatCompletionsResponse<ChatCompletionsChunk>>\n    >;\n  return messages.pipe(\n    // Filter out messages that don't have the expected structure\n    filter((event) => {\n      // Skip messages with null choices\n      if (!event.message.choices) {\n        return false;\n      }\n      return true;\n    }),\n    tap((event) => {\n      if (isErrorResponse(event.message)) {\n        throw new Error(event.message.error);\n      }\n    }),\n    // Stop the stream when we get a done message or when the finish_reason is \"stop\"\n    takeWhile((event) => {\n      // If it's an error response, we should continue to let the tap operator handle it\n      if (isErrorResponse(event.message)) {\n        return true;\n      }\n      \n      // Check for the explicit done message\n      if (event.message.choices && \n          event.message.choices[0].delta && \n          'done' in event.message.choices[0].delta && \n          event.message.choices[0].delta.done === true) {\n        return false;\n      }\n      \n      // Check for finish_reason = \"stop\"\n      if (event.message.choices && \n          'finish_reason' in event.message.choices[0] &&\n          event.message.choices[0].finish_reason === \"stop\") {\n        return false;\n      }\n      \n      return true;\n    }),\n    map((event) => event.message),\n  );\n}\n\nlet loggedWarning = false;\n\n/** Check if the LLM provider API is enabled via the LLM plugin. */\nexport const health = async (): Promise<LLMProviderHealthDetails> => {\n  // First check if the plugin is enabled.\n  try {\n    const settings = await getBackendSrv().get(`${LLM_PLUGIN_ROUTE}/settings`, undefined, undefined, {\n      showSuccessAlert: false,\n      showErrorAlert: false,\n    });\n    if (!settings.enabled) {\n      return { configured: false, ok: false, error: 'The Grafana LLM plugin is not enabled.' };\n    }\n  } catch (e) {\n    logDebug(String(e));\n    logDebug(\n      'Failed to check if LLM provider is enabled. This is expected if the Grafana LLM plugin is not installed, and the above error can be ignored.'\n    );\n    loggedWarning = true;\n    return { configured: false, ok: false, error: 'The Grafana LLM plugin is not installed.' };\n  }\n\n  // Run a health check to see if the LLM provider is configured on the plugin.\n  let response: HealthCheckResponse;\n  try {\n    response = await getBackendSrv().get(`${LLM_PLUGIN_ROUTE}/health`, undefined, undefined, {\n      showSuccessAlert: false,\n      showErrorAlert: false,\n    });\n  } catch (e) {\n    if (!loggedWarning) {\n      logDebug(String(e));\n      logDebug(\n        'Failed to check if LLM provider is enabled. This is expected if the Grafana LLM plugin is not installed, and the above error can be ignored.'\n      );\n      loggedWarning = true;\n    }\n    return { configured: false, ok: false, error: 'The Grafana LLM plugin is not installed.' };\n  }\n\n  const { details } = response;\n  // Update the version if it's present on the response.\n  if (details?.version !== undefined) {\n    setLLMPluginVersion(details.version);\n  }\n  if (details?.llmProvider === undefined) {\n    return { configured: false, ok: false, error: 'The Grafana LLM plugin is outdated; please update it.' };\n  }\n  return typeof details.llmProvider === 'boolean' ? { configured: details.llmProvider, ok: details.llmProvider } : details.llmProvider;\n};\n\nexport const enabled = async (): Promise<boolean> => {\n  const healthDetails = await health();\n  return healthDetails.configured && healthDetails.ok;\n};\n\n/**\n * Enum representing different states for a stream.\n * @enum {string}\n */\nexport enum StreamStatus {\n  IDLE = 'idle',\n  GENERATING = 'generating',\n  COMPLETED = 'completed',\n}\n\n/**\n * A constant representing the timeout value in milliseconds.\n * @type {number}\n */\nexport const TIMEOUT = 10000;\n\n/**\n * A type representing the state of an LLM stream.\n * @typedef {Object} LLMStreamState\n * @property {React.Dispatch<React.SetStateAction<Message[]>} setMessages - A function to set messages.\n * @property {string} reply - The reply associated with the stream.\n * @property {typeof StreamStatus} streamStatus - The current status of the stream.\n * @property {Error|undefined} error - An optional error associated with the stream.\n * @property {{\n *    enabled: boolean|undefined;\n *    stream?: undefined;\n *  }|{\n *    enabled: boolean|undefined;\n *    stream: Subscription;\n *  }|undefined} value - A value that can be an object with 'enabled' and 'stream' properties or undefined.\n */\nexport type LLMStreamState = {\n  setMessages: React.Dispatch<React.SetStateAction<Message[]>>;\n  reply: string;\n  streamStatus: StreamStatus;\n  error: Error | undefined;\n  value:\n  | {\n    enabled: boolean | undefined;\n    stream?: undefined;\n  }\n  | {\n    enabled: boolean | undefined;\n    stream: Subscription;\n  }\n  | undefined;\n};\n\n/**\n * A custom React hook for managing an LLM stream that communicates with the provided model.\n *\n * @param {string} [model=Model.LARGE] - The LLM model to use for communication.\n * @param {number} [temperature=1] - The temperature value for text generation (default is 1).\n * @param {function} [notifyError] - A callback function for handling errors.\n *\n * @returns {LLMStreamState} - An object containing the state of the LLM stream.\n * @property {function} setMessages - A function to update the list of messages in the stream.\n * @property {string} reply - The most recent reply received from the LLM stream.\n * @property {StreamStatus} streamStatus - The status of the stream (\"idle\", \"generating\" or \"completed\").\n * @property {Error|undefined} error - An error object if an error occurs, or undefined if no error.\n * @property {object|undefined} value - The current value of the stream.\n * @property {boolean|undefined} value.enabled - Indicates whether the stream is enabled (true or false).\n * @property {Subscription|undefined} value.stream - The stream subscription object if the stream is active, or undefined if not.\n */\nexport function useLLMStream(\n  model = Model.LARGE,\n  temperature = 1,\n  notifyError: (title: string, text?: string, traceId?: string) => void = () => {}\n): LLMStreamState {\n  // The messages array to send to the LLM.\n  const [messages, setMessages] = useState<Message[]>([]);\n  // The latest reply from the LLM.\n  const [reply, setReply] = useState('');\n  const [streamStatus, setStreamStatus] = useState<StreamStatus>(StreamStatus.IDLE);\n  const [error, setError] = useState<Error>();\n\n  const onError = useCallback(\n    (e: Error) => {\n      setStreamStatus(StreamStatus.IDLE);\n      setMessages([]);\n      setError(e);\n      notifyError(\n        'Failed to generate content using LLM provider',\n        `Please try again or if the problem persists, contact your organization admin.`\n      );\n      console.error(e);\n    },\n    [notifyError]\n  );\n\n  const { error: enabledError, value: isEnabled } = useAsync(async () => await enabled(), [enabled]);\n\n  const { error: asyncError, value } = useAsync(async () => {\n    if (!isEnabled || !messages.length) {\n      return { enabled: isEnabled };\n    }\n\n    setStreamStatus(StreamStatus.GENERATING);\n    setError(undefined);\n    // Stream the completions. Each element is the next stream chunk.\n    const stream = streamChatCompletions({\n      model,\n      temperature,\n      messages,\n    }).pipe(\n      // Accumulate the stream content into a stream of strings, where each\n      // element contains the accumulated message so far.\n      accumulateContent()\n      // The stream is just a regular Observable, so we can use standard rxjs\n      // functionality to update state, e.g. recording when the stream\n      // has completed.\n      // The operator decision tree on the rxjs website is a useful resource:\n      // https://rxjs.dev/operator-decision-tree.)\n    );\n    // Subscribe to the stream and update the state for each returned value.\n    return {\n      enabled: isEnabled,\n      stream: stream.subscribe({\n        next: setReply,\n        error: onError,\n        complete: () => {\n          setStreamStatus(StreamStatus.COMPLETED);\n          setTimeout(() => {\n            setStreamStatus(StreamStatus.IDLE);\n          });\n          setMessages([]);\n          setError(undefined);\n        },\n      }),\n    };\n  }, [messages, isEnabled]);\n\n  // Unsubscribe from the stream when the component unmounts.\n  useEffect(() => {\n    return () => {\n      if (value?.stream) {\n        value.stream.unsubscribe();\n      }\n    };\n  }, [value]);\n\n  // If the stream is generating and we haven't received a reply, it times out.\n  useEffect(() => {\n    let timeout: NodeJS.Timeout | undefined;\n    if (streamStatus === StreamStatus.GENERATING && reply === '') {\n      timeout = setTimeout(() => {\n        onError(new Error(`LLM stream timed out after ${TIMEOUT}ms`));\n      }, TIMEOUT);\n    }\n    return () => {\n      timeout && clearTimeout(timeout);\n    };\n  }, [streamStatus, reply, onError]);\n\n  if (asyncError || enabledError) {\n    setError(asyncError || enabledError);\n  }\n\n  return {\n    setMessages,\n    reply,\n    streamStatus,\n    error,\n    value,\n  };\n}\n\n/**\n * An rxjs operator that accumulates tool call messages from a stream of chat completion responses into a complete tool call message.\n * \n * @returns An observable that emits the accumulated tool call message when complete.\n * @example\n * const stream = streamChatCompletions({...}).pipe(\n *   accumulateToolCalls()\n * );\n * stream.subscribe({\n *   next: (toolCallMessage) => console.log('Received complete tool call:', toolCallMessage),\n *   error: console.error\n * });\n */\nexport function accumulateToolCalls(): UnaryFunction<\n  Observable<ChatCompletionsResponse<ChatCompletionsChunk>>,\n  Observable<ToolCallsMessage>\n> {\n  return pipe(\n    filter((response: ChatCompletionsResponse<ChatCompletionsChunk>) => isToolCallsMessage(response.choices[0].delta)),\n    // Collect all tool call chunks\n    toArray(),\n    // Process the array to reconstruct the complete tool call message\n    map((responses: Array<ChatCompletionsResponse<ChatCompletionsChunk>>) => {\n      const toolCallChunks = responses.map(r => r.choices[0].delta as ToolCallsMessage);\n      return recoverToolCallMessage(toolCallChunks);\n    })\n  );\n}\n\n/**\n * Recovers a complete tool call message from individual chunks.\n * \n * @param toolCallMessages - Array of tool call message chunks\n * @returns A complete tool call message with all chunks combined\n */\nexport function recoverToolCallMessage(toolCallMessages: ToolCallsMessage[]): ToolCallsMessage {\n  const recoveredToolCallMessage: ToolCallsMessage = {\n    role: 'assistant',\n    tool_calls: [],\n  };\n\n  for (const msg of toolCallMessages) {\n    for (const tc of msg.tool_calls) {\n      if (tc.index! >= recoveredToolCallMessage.tool_calls.length) {\n        recoveredToolCallMessage.tool_calls.push({ \n          ...tc, \n          function: { ...tc.function, arguments: tc.function.arguments ?? '' } \n        });\n      } else {\n        recoveredToolCallMessage.tool_calls[tc.index!].function.arguments += tc.function.arguments ?? '';\n      }\n    }\n  }\n\n  // Ensure final arguments are never empty\n  for (const tc of recoveredToolCallMessage.tool_calls) {\n    if (!tc.function.arguments) {\n      tc.function.arguments = '{}';\n    }\n  }\n\n  return recoveredToolCallMessage;\n}\n","/**\n * @deprecated This module is deprecated and will be removed in a future version.\n * Please use the vendor-neutral `llm.ts` module instead.\n * \n * All exports from this file are re-exported from `llm.ts` for backward compatibility.\n * \n * BREAKING CHANGE in v0.13.0: The health check response format has changed from\n * { details: { openAI: { configured: true, ok: true } } }\n * to\n * { details: { llmProvider: { configured: true, ok: true } } }\n * \n * This module now handles both formats for backward compatibility, but will be removed in a future version.\n */\n\nimport { getBackendSrv } from '@grafana/runtime';\nimport { LLM_PLUGIN_ROUTE } from './constants';\n\n// Re-export everything from llm.ts except enabled\nexport * from './llm';\n\n// Override enabled function to handle both old and new formats\nexport const enabled = async (): Promise<boolean> => {\n  try {\n    const settings = await getBackendSrv().get(`${LLM_PLUGIN_ROUTE}/settings`);\n    if (!settings.enabled) {\n      return false;\n    }\n\n    const health = await getBackendSrv().get(`${LLM_PLUGIN_ROUTE}/health`);\n    const details = health.details;\n\n    // Handle both new and old formats\n    if (details.llmProvider) {\n      return details.llmProvider.configured && details.llmProvider.ok;\n    }\n    if (details.openAI) {\n      return details.openAI.configured && details.openAI.ok;\n    }\n    return false;\n  } catch (e) {\n    return false;\n  }\n};\n","import React from 'react';\n\nimport { isLiveChannelMessageEvent, LiveChannelAddress, LiveChannelMessageEvent, LiveChannelScope } from '@grafana/data';\nimport { getGrafanaLiveSrv, GrafanaLiveSrv } from '@grafana/runtime';\nimport { Transport } from '@modelcontextprotocol/sdk/shared/transport';\nimport { Client } from '@modelcontextprotocol/sdk/client/index';\nimport { JSONRPCMessage, JSONRPCMessageSchema, Tool as MCPTool } from '@modelcontextprotocol/sdk/types';\nimport { Observable, filter } from 'rxjs';\nimport { v4 as uuid } from 'uuid';\n\nimport { LLM_PLUGIN_ID } from './constants';\nimport { Tool as OpenAITool } from './openai';\n\nconst MCP_GRAFANA_PATH = 'mcp/grafana'\n\n/**\n * An MCP transport which uses the Grafana LLM plugin's built-in MCP server,\n * over Grafana Live.\n *\n * Use this with a client from `@modelcontextprotocol/sdk`.\n *\n * @experimental\n */\nexport class GrafanaLiveTransport implements Transport {\n  _grafanaLiveSrv: GrafanaLiveSrv = getGrafanaLiveSrv()\n\n  /**\n   * The Grafana Live channel used by this transport.\n   */\n  _subscribeChannel: LiveChannelAddress;\n\n  /**\n   * The Grafana Live channel used by this transport.\n   */\n  _publishChannel: LiveChannelAddress;\n\n  /**\n   * The Grafana Live stream over which MCP messages are received.\n   */\n  _stream?: Observable<LiveChannelMessageEvent<unknown>>;\n\n  // Methods defined as part of the Transport interface.\n  // These will be attached by the client.\n  onclose?: (() => void) | undefined;\n  onerror?: ((error: Error) => void) | undefined;\n  onmessage?: ((message: JSONRPCMessage) => void) | undefined;\n\n  constructor(path?: string) {\n    if (path === undefined) {\n      // Construct a unique path for this transport.\n      const pathId = uuid();\n      path = `${MCP_GRAFANA_PATH}/${pathId}`;\n    }\n    this._subscribeChannel = {\n      scope: LiveChannelScope.Plugin,\n      namespace: LLM_PLUGIN_ID,\n      path: `${path}/subscribe`,\n    };\n    this._publishChannel = {\n      scope: LiveChannelScope.Plugin,\n      namespace: LLM_PLUGIN_ID,\n      path: `${path}/publish`,\n    };\n  }\n\n  async start(): Promise<void> {\n    if (this._stream !== undefined) {\n      throw new Error(\n        \"GrafanaLiveTransport already started! If using Client class, note that connect() calls start() automatically.\"\n      );\n    }\n\n    const stream = this._grafanaLiveSrv.getStream(this._subscribeChannel)\n      .pipe(filter((event) => isLiveChannelMessageEvent(event)));\n    this._stream = stream;\n    stream.subscribe((event) => {\n      let message: JSONRPCMessage;\n      try {\n        message = JSONRPCMessageSchema.parse(event.message);\n      } catch (error) {\n        this.onerror?.(error as Error)\n        return;\n      }\n      this.onmessage?.(message);\n    });\n  }\n\n  async send(message: JSONRPCMessage): Promise<void> {\n    if (this._stream === undefined) {\n      throw new Error(\"not connected\");\n    }\n\n    // The Grafana Live service API for publishing messages sends a message\n    // to Grafana's HTTP API rather than over the live channel, for reasons\n    // that are unclear (but presumably justified in the default case).\n    // This is fine when there is only one Grafana instance, but when there\n    // are multiple (e.g. in a HA setup), the HTTP request will be routed\n    // to a random Grafana instance, while we need it to be routed to the\n    // same instance that the client is connected to (since there is a\n    // long-lived stream over the live channel).\n    //\n    // We can use the `useSocket` argument when trying to publish to the\n    // live channel to force the use of the Websocket instead of the HTTP API.\n    // This will work in both single-instance and HA setups. However, it's only\n    // available in Grafana 11.6.0 and later. We can check for this by checking\n    // if the `publish` method has a third argument, which is the `options`\n    // argument.\n    const hasPublishOptions = this._grafanaLiveSrv.publish?.length >= 3;\n    if (hasPublishOptions) {\n      // TODO: use `LivePublishOptions` from `@grafana/runtime` once\n      // Grafana 11.6.0 is released. We can remove these `@ts-expect-error`\n      // comments once that happens.\n      //@ts-expect-error\n      const options: LivePublishOptions = { useSocket: true };\n      //@ts-expect-error\n      return this._grafanaLiveSrv.publish(this._publishChannel, message, options);\n    }\n\n    // If that option isn't available, we can first fall back to trying to\n    // drilling down into the implementation details of the Grafana Live\n    // service and using the Centrifuge API directly to publish the message\n    // to the same stream that the client is connected to.\n    // Realistically this should work in all versions of Grafana older than\n    // 9, which is much further back than this plugin even supports, so should\n    // always work.\n    const centrifugeSubscription =\n      // @ts-expect-error\n      this._grafanaLiveSrv.deps?.centrifugeSrv?.getChannel?.(\n        this._publishChannel,\n      )?.subscription;\n    if (centrifugeSubscription) {\n      return centrifugeSubscription.publish(message);\n    }\n\n    // If the centrifuge subscription is still not available for some reason,\n    // fall back to the official HTTP publish method. This won't work in HA\n    // setups but it's better than nothing.\n    console.warn(\n      \"Websocket subscription not available, falling back to HTTP publish. \" +\n      \"This may fail in HA setups. If you see this, please create an issue at \" +\n      \"https://github.com/grafana/grafana-llm-app/issues/new.\"\n    );\n    await this._grafanaLiveSrv.publish(this._publishChannel, message);\n  }\n\n  async close(): Promise<void> {\n    this._stream = undefined;\n  }\n}\n\n// Create a map to store client instances. These will be keyed by the appName and appVersion.\n// This effectively means:\n// - each app will have a single client instance that is reused across the application.\n// - since clients are stored outside of the MCPClientProvider component, they will be\n//   cleaned up when the component unmounts.\n// - this also allows users to wrap the MCPClientProvider in Suspense, which will\n//   automatically suspend the component until the client is ready.\nconst clientMap = new Map<string, Client>();\n\n// Context holding a client instance.\nconst MCPClientContext = React.createContext<Client | null>(null);\n\n// Create a key for the client map.\nfunction clientKey(appName: string, appVersion: string) {\n  return `${appName}-${appVersion}`;\n}\n\n// A resource type, used with `createClientResource` to fetch the client or\n// throw a promise if it's not yet ready.\ntype ClientResource = {\n  read: () => Client;\n};\n\n// Create a resource that works with Suspense.\nfunction createClientResource(appName: string, appVersion: string): ClientResource {\n  let status: 'pending' | 'success' | 'error' = 'pending';\n  let result: Client | null = null;\n  let error: Error | null = null;\n\n  const key = clientKey(appName, appVersion);\n  const promise = (async () => {\n    if (clientMap.has(key)) {\n      result = clientMap.get(key)!;\n      status = 'success';\n      return result;\n    }\n\n    try {\n      const client = new Client({\n        name: appName,\n        version: appVersion,\n      });\n      const transport = new GrafanaLiveTransport();\n      await client.connect(transport);\n      clientMap.set(key, client);\n      status = 'success';\n      result = client;\n      return client;\n    } catch (e) {\n      status = 'error';\n      error = e as Error;\n      throw e;\n    }\n  })();\n\n  return {\n    read() {\n      if (status === 'pending') {\n        throw promise;\n      } else if (status === 'error') {\n        throw error;\n      } else if (status === 'success' && result) {\n        return result;\n      }\n      throw new Error('Unexpected resource state');\n    },\n  };\n}\n\ninterface MCPClientProviderProps {\n  appName: string;\n  appVersion: string;\n  children: React.ReactNode;\n}\n\n/**\n * MCPClientProvider is a React context provider that creates an MCP client\n * and manages its lifecycle.\n *\n * It should be used to wrap the entire application in a single provider.\n * This ensures that the client is created once and reused across the application.\n *\n * It also supports Suspense, which will suspend the component until the client\n * is ready. This allows you to use the client in components that are not yet\n * ready, such as those that are loading data.\n *\n * Example usage:\n * ```tsx\n * <Suspense fallback={<LoadingPlaceholder />}>\n *   <ErrorBoundary>\n *     {({ error }) => {\n *       if (error) {\n *         return <div>Something went wrong: {error.message}</div>;\n *       }\n *       return (\n *         <MCPClientProvider appName=\"MyApp\" appVersion=\"1.0.0\">\n *           <YourComponent />\n *         </MCPClientProvider>\n *       );\n *     }}\n *   </ErrorBoundary>\n * </Suspense>\n * ```\n *\n * @experimental\n */\nexport function MCPClientProvider({\n  appName,\n  appVersion,\n  children,\n}: MCPClientProviderProps) {\n  const resource = createClientResource(appName, appVersion);\n\n  // This will either return the client or throw a promise/error.\n  // If it throws a promise, Suspense will suspend the component until it resolves.\n  // If it throws an error, it should be caught by an ErrorBoundary.\n  const client = resource.read();\n\n  // Cleanup when the component unmounts.\n  React.useEffect(() => {\n    return () => {\n      if (client) {\n        client.close();\n      }\n      clientMap.delete(clientKey(appName, appVersion));\n    };\n  }, [client, appName, appVersion]);\n\n  return (\n    <MCPClientContext.Provider value={client}>\n      {children}\n    </MCPClientContext.Provider>\n  );\n}\n\n/**\n * Convenience hook to use an MCP client from a component.\n *\n * This hook should be used within an `MCPClientProvider`.\n *\n * @experimental\n */\nexport function useMCPClient(): Client {\n  const client = React.useContext(MCPClientContext);\n  if (client === null) {\n    throw new Error('useMCPClient must be used within an MCPClientProvider');\n  }\n  return client;\n}\n\n/**\n * Re-export of the Client class from the MCP SDK.\n *\n * @experimental\n */\nexport { Client };\n\n/**\n * Convert an array of MCP tools to an array of OpenAI tools.\n *\n * This is useful when you want to use the MCP client with the LLM plugin's\n * `chatCompletions` or `streamChatCompletions` functions.\n *\n * @experimental\n */\nexport function convertToolsToOpenAI(tools: MCPTool[]): OpenAITool[] {\n  return tools.map(convertToolToOpenAI);\n}\n\nfunction convertToolToOpenAI(tool: MCPTool): OpenAITool {\n  return {\n    type: 'function',\n    function: {\n      name: tool.name,\n      description: tool.description,\n      parameters: tool.inputSchema.properties !== undefined ? tool.inputSchema : undefined,\n    },\n  };\n}\n","/**\n * Vector search API.\n *\n * This module can be used to interact with the vector database configured\n * in the Grafana LLM app plugin. That plugin must be installed, enabled and configured\n * in order for these functions to work.\n *\n * The {@link enabled} function can be used to check if the plugin is enabled and configured.\n */\n\nimport { getBackendSrv, logDebug } from '@grafana/runtime';\nimport { LLM_PLUGIN_ROUTE, setLLMPluginVersion } from './constants';\nimport { HealthCheckResponse, VectorHealthDetails } from './types';\n\ninterface SearchResultPayload extends Record<string, any> {}\n\n/**\n * A request to search for resources in the vector database.\n **/\nexport interface SearchRequest {\n  /**\n   * The name of the collection to search in.\n   **/\n  collection: string;\n\n  /** The query to search for. */\n  query: string;\n\n  /**\n   * Limit the number of results returned to the top `topK` results.\n   *\n   * Defaults to 10.\n   **/\n  topK?: number;\n\n  /** Metadata filters to apply to the vector search. */\n  /* example: filter: { metric_type: { $eq: 'histogram' } } */\n  filter?: Record<string, any>;\n}\n\n/**\n * The results of a vector search.\n *\n * Results will be ordered by score, descending.\n */\nexport interface SearchResult<T extends SearchResultPayload> {\n  /**\n   * The payload of the result.\n   *\n   * The type of this payload depends on the collection that was searched in.\n   * Grafana core types will be added to the same module as this type as they\n   * are implemented.\n   **/\n  payload: T;\n\n  /**\n   * The score of the result.\n   *\n   * This is a number between 0 and 1, where 1 is the best possible match.\n   */\n  score: number;\n}\n\ninterface SearchResultResponse<T extends SearchResultPayload> {\n  results: Array<SearchResult<T>>;\n}\n\n/**\n * Search for resources in the configured vector database.\n */\nexport async function search<T extends SearchResultPayload>(request: SearchRequest): Promise<Array<SearchResult<T>>> {\n  const response = await getBackendSrv().post<SearchResultResponse<T>>(\n    '/api/plugins/grafana-llm-app/resources/vector/search',\n    request,\n    {\n      headers: { 'Content-Type': 'application/json' },\n    }\n  );\n  return response.results;\n}\n\nlet loggedWarning = false;\n\n/** Check if the vector API is enabled and configured via the LLM plugin. */\nexport const health = async (): Promise<VectorHealthDetails> => {\n  // First check if the plugin is enabled.\n  try {\n    const settings = await getBackendSrv().get(`${LLM_PLUGIN_ROUTE}/settings`, undefined, undefined, {\n      showSuccessAlert: false,\n      showErrorAlert: false,\n    });\n    if (!settings.enabled) {\n      return { enabled: false, ok: false, error: 'The Grafana LLM plugin is not enabled.' };\n    }\n  } catch (e) {\n    logDebug(String(e));\n    logDebug(\n      'Failed to check if the vector service is enabled. This is expected if the Grafana LLM plugin is not installed, and the above error can be ignored.'\n    );\n    loggedWarning = true;\n    return { enabled: false, ok: false, error: 'The Grafana LLM plugin is not installed.' };\n  }\n\n  // Run a health check to see if the vector service is configured on the plugin.\n  let response: HealthCheckResponse;\n  try {\n    response = await getBackendSrv().get(`${LLM_PLUGIN_ROUTE}/health`, undefined, undefined, {\n      showSuccessAlert: false,\n      showErrorAlert: false,\n    });\n  } catch (e) {\n    // We shouldn't really get here if we managed to get the plugin's settings above,\n    // but catch this just in case.\n    if (!loggedWarning) {\n      logDebug(String(e));\n      logDebug(\n        'Failed to check if vector service is enabled. This is expected if the Grafana LLM plugin is not installed, and the above error can be ignored.'\n      );\n      loggedWarning = true;\n    }\n    return { enabled: false, ok: false, error: 'The Grafana LLM plugin is not installed.' };\n  }\n\n  const { details } = response;\n  // Update the version if it's present on the response.\n  if (details?.version !== undefined) {\n    setLLMPluginVersion(details.version);\n  }\n  if (details?.vector === undefined) {\n    return { enabled: false, ok: false, error: 'The Grafana LLM plugin is outdated; please update it.' };\n  }\n  return typeof details.vector === 'boolean' ? { enabled: details.vector, ok: details.vector } : details.vector;\n};\n\nexport const enabled = async (): Promise<boolean> => {\n  const healthDetails = await health();\n  return healthDetails.enabled && healthDetails.ok;\n};\n"],"names":["SemVer","logWarning","Model","pipe","filter","map","scan","getBackendSrv","LiveChannelScope","uuidv4","getGrafanaLiveSrv","isLiveChannelMessageEvent","tap","takeWhile","loggedWarning","health","logDebug","enabled","StreamStatus","useState","useCallback","useAsync","useEffect","toArray","uuid","JSONRPCMessageSchema","Client"],"mappings":";;;;;;;;;;;;;AAIO,MAAM,aAAgB,GAAA,iBAAA;AAChB,MAAA,gBAAA,GAAmB,gBAAgB,aAAa,CAAA,CAAA;AAKlD,IAAA,kBAAA,GAAqB,IAAIA,aAAA,CAAO,OAAO,CAAA;AAE3C,SAAS,oBAAoB,OAAiB,EAAA;AACnD,EAAI,IAAA;AACF,IAAqB,kBAAA,GAAA,IAAIA,cAAO,OAAO,CAAA;AAAA,WAChC,CAAG,EAAA;AACV,IAAAC,kBAAA,CAAW,8EAA8E,CAAA;AAAA;AAE7F;;ACSA,MAAM,yBAA4B,GAAA,yBAAA;AAgGtB,IAAA,KAAA,qBAAAC,MAAL,KAAA;AACL,EAAAA,OAAA,MAAO,CAAA,GAAA,MAAA;AACP,EAAAA,OAAA,OAAQ,CAAA,GAAA,OAAA;AAFE,EAAAA,OAAAA,MAAAA;AAAA,CAAA,EAAA,KAAA,IAAA,EAAA,CAAA;AAmLL,SAAS,iBAAiB,OAA0D,EAAA;AACzF,EAAA,OAAO,SAAa,IAAA,OAAA;AACtB;AAGO,SAAS,cAAc,OAAuD,EAAA;AACnF,EAAO,OAAA,MAAA,IAAU,OAAW,IAAA,OAAA,CAAQ,IAAQ,IAAA,IAAA;AAC9C;AAGO,SAAS,gBACd,QAC0C,EAAA;AAC1C,EAAA,OAAO,OAAW,IAAA,QAAA;AACpB;AAGO,SAAS,sBAAsB,OAA+D,EAAA;AACnG,EAAO,OAAA,MAAA,IAAU,WAAW,WAAe,IAAA,OAAA;AAC7C;AAGO,SAAS,mBAAmB,OAA4D,EAAA;AAC7F,EAAO,OAAA,YAAA,IAAgB,OAAW,IAAA,OAAA,CAAQ,UAAc,IAAA,IAAA;AAC1D;AAgBO,SAAS,cAGd,GAAA;AACA,EAAO,OAAAC,SAAA;AAAA,IACLC,gBAAA,CAAO,CAAC,QAA4D,KAAA,gBAAA,CAAiB,SAAS,OAAQ,CAAA,CAAC,CAAE,CAAA,KAAK,CAAC,CAAA;AAAA;AAAA,IAE/GC,aAAA;AAAA,MACE,CAAC,QAA6D,KAAA,QAAA,CAAS,OAAQ,CAAA,CAAC,EAAE,KAAyB,CAAA;AAAA;AAC7G,GACF;AACF;AAgBO,SAAS,iBAGd,GAAA;AACA,EAAO,OAAAF,SAAA;AAAA,IACL,cAAe,EAAA;AAAA,IACfG,eAAK,CAAC,GAAA,EAAK,IAAS,KAAA,GAAA,GAAM,MAAM,EAAE;AAAA,GACpC;AACF;AAKA,eAAsB,gBAAgB,OAAmE,EAAA;AACvG,EAAM,MAAA,QAAA,GAAW,MAAMC,qBAAA,EAAgB,CAAA,IAAA;AAAA,IACrC,0CAA0C,yBAAyB,CAAA,CAAA;AAAA,IACnE,OAAA;AAAA,IACA;AAAA,MACE,OAAA,EAAS,EAAE,cAAA,EAAgB,kBAAmB;AAAA;AAChD,GACF;AACA,EAAO,OAAA,QAAA;AACT;AA6BO,SAAS,sBACd,OAC2D,EAAA;AAC3D,EAAA,MAAM,OAA8B,GAAA;AAAA,IAClC,OAAOC,qBAAiB,CAAA,MAAA;AAAA,IACxB,SAAW,EAAA,aAAA;AAAA,IACX,IAAA,EAAM,yBAA4B,GAAA,GAAA,GAAMC,OAAO,EAAA;AAAA,IAC/C,IAAM,EAAA;AAAA,GACR;AACA,EAAA,MAAM,QAAW,GAAAC,yBAAA,EACd,CAAA,SAAA,CAAU,OAAO,CAAA,CACjB,IAAK,CAAAN,gBAAA,CAAO,CAAC,KAAA,KAAUO,8BAA0B,CAAA,KAAK,CAAC,CAAC,CAAA;AAG3D,EAAA,OAAO,QAAS,CAAA,IAAA;AAAA;AAAA,IAEdP,gBAAA,CAAO,CAAC,KAAU,KAAA;AAEhB,MAAI,IAAA,CAAC,KAAM,CAAA,OAAA,CAAQ,OAAS,EAAA;AAC1B,QAAO,OAAA,KAAA;AAAA;AAET,MAAO,OAAA,IAAA;AAAA,KACR,CAAA;AAAA,IACDQ,aAAA,CAAI,CAAC,KAAU,KAAA;AACb,MAAI,IAAA,eAAA,CAAgB,KAAM,CAAA,OAAO,CAAG,EAAA;AAClC,QAAA,MAAM,IAAI,KAAA,CAAM,KAAM,CAAA,OAAA,CAAQ,KAAK,CAAA;AAAA;AACrC,KACD,CAAA;AAAA;AAAA,IAEDC,mBAAA,CAAU,CAAC,KAAU,KAAA;AAEnB,MAAI,IAAA,eAAA,CAAgB,KAAM,CAAA,OAAO,CAAG,EAAA;AAClC,QAAO,OAAA,IAAA;AAAA;AAIT,MAAI,IAAA,KAAA,CAAM,QAAQ,OACd,IAAA,KAAA,CAAM,QAAQ,OAAQ,CAAA,CAAC,CAAE,CAAA,KAAA,IACzB,MAAU,IAAA,KAAA,CAAM,QAAQ,OAAQ,CAAA,CAAC,CAAE,CAAA,KAAA,IACnC,KAAM,CAAA,OAAA,CAAQ,QAAQ,CAAC,CAAA,CAAE,KAAM,CAAA,IAAA,KAAS,IAAM,EAAA;AAChD,QAAO,OAAA,KAAA;AAAA;AAIT,MAAA,IAAI,KAAM,CAAA,OAAA,CAAQ,OACd,IAAA,eAAA,IAAmB,MAAM,OAAQ,CAAA,OAAA,CAAQ,CAAC,CAAA,IAC1C,MAAM,OAAQ,CAAA,OAAA,CAAQ,CAAC,CAAA,CAAE,kBAAkB,MAAQ,EAAA;AACrD,QAAO,OAAA,KAAA;AAAA;AAGT,MAAO,OAAA,IAAA;AAAA,KACR,CAAA;AAAA,IACDR,aAAI,CAAA,CAAC,KAAU,KAAA,KAAA,CAAM,OAAO;AAAA,GAC9B;AACF;AAEA,IAAIS,eAAgB,GAAA,KAAA;AAGb,MAAMC,WAAS,YAA+C;AAEnE,EAAI,IAAA;AACF,IAAM,MAAA,QAAA,GAAW,MAAMR,qBAAc,EAAA,CAAE,IAAI,CAAG,EAAA,gBAAgB,CAAa,SAAA,CAAA,EAAA,KAAA,CAAA,EAAW,KAAW,CAAA,EAAA;AAAA,MAC/F,gBAAkB,EAAA,KAAA;AAAA,MAClB,cAAgB,EAAA;AAAA,KACjB,CAAA;AACD,IAAI,IAAA,CAAC,SAAS,OAAS,EAAA;AACrB,MAAA,OAAO,EAAE,UAAY,EAAA,KAAA,EAAO,EAAI,EAAA,KAAA,EAAO,OAAO,wCAAyC,EAAA;AAAA;AACzF,WACO,CAAG,EAAA;AACV,IAASS,gBAAA,CAAA,MAAA,CAAO,CAAC,CAAC,CAAA;AAClB,IAAAA,gBAAA;AAAA,MACE;AAAA,KACF;AACA,IAAgBF,eAAA,GAAA,IAAA;AAChB,IAAA,OAAO,EAAE,UAAY,EAAA,KAAA,EAAO,EAAI,EAAA,KAAA,EAAO,OAAO,0CAA2C,EAAA;AAAA;AAI3F,EAAI,IAAA,QAAA;AACJ,EAAI,IAAA;AACF,IAAW,QAAA,GAAA,MAAMP,uBAAgB,CAAA,GAAA,CAAI,GAAG,gBAAgB,CAAA,OAAA,CAAA,EAAW,QAAW,KAAW,CAAA,EAAA;AAAA,MACvF,gBAAkB,EAAA,KAAA;AAAA,MAClB,cAAgB,EAAA;AAAA,KACjB,CAAA;AAAA,WACM,CAAG,EAAA;AACV,IAAA,IAAI,CAACO,eAAe,EAAA;AAClB,MAASE,gBAAA,CAAA,MAAA,CAAO,CAAC,CAAC,CAAA;AAClB,MAAAA,gBAAA;AAAA,QACE;AAAA,OACF;AACA,MAAgBF,eAAA,GAAA,IAAA;AAAA;AAElB,IAAA,OAAO,EAAE,UAAY,EAAA,KAAA,EAAO,EAAI,EAAA,KAAA,EAAO,OAAO,0CAA2C,EAAA;AAAA;AAG3F,EAAM,MAAA,EAAE,SAAY,GAAA,QAAA;AAEpB,EAAI,IAAA,OAAA,EAAS,YAAY,SAAW,EAAA;AAClC,IAAA,mBAAA,CAAoB,QAAQ,OAAO,CAAA;AAAA;AAErC,EAAI,IAAA,OAAA,EAAS,gBAAgB,SAAW,EAAA;AACtC,IAAA,OAAO,EAAE,UAAY,EAAA,KAAA,EAAO,EAAI,EAAA,KAAA,EAAO,OAAO,uDAAwD,EAAA;AAAA;AAExG,EAAA,OAAO,OAAO,OAAA,CAAQ,WAAgB,KAAA,SAAA,GAAY,EAAE,UAAA,EAAY,OAAQ,CAAA,WAAA,EAAa,EAAI,EAAA,OAAA,CAAQ,WAAY,EAAA,GAAI,OAAQ,CAAA,WAAA;AAC3H,CAAA;AAEO,MAAMG,YAAU,YAA8B;AACnD,EAAM,MAAA,aAAA,GAAgB,MAAMF,QAAO,EAAA;AACnC,EAAO,OAAA,aAAA,CAAc,cAAc,aAAc,CAAA,EAAA;AACnD,CAAA;AAMY,IAAA,YAAA,qBAAAG,aAAL,KAAA;AACL,EAAAA,cAAA,MAAO,CAAA,GAAA,MAAA;AACP,EAAAA,cAAA,YAAa,CAAA,GAAA,YAAA;AACb,EAAAA,cAAA,WAAY,CAAA,GAAA,WAAA;AAHF,EAAAA,OAAAA,aAAAA;AAAA,CAAA,EAAA,YAAA,IAAA,EAAA,CAAA;AAUL,MAAM,OAAU,GAAA,GAAA;AAkDhB,SAAS,aACd,KAAQ,GAAA,OAAA,cACR,WAAc,GAAA,CAAA,EACd,cAAwE,MAAM;AAAC,CAC/D,EAAA;AAEhB,EAAA,MAAM,CAAC,QAAU,EAAA,WAAW,CAAI,GAAAC,cAAA,CAAoB,EAAE,CAAA;AAEtD,EAAA,MAAM,CAAC,KAAA,EAAO,QAAQ,CAAA,GAAIA,eAAS,EAAE,CAAA;AACrC,EAAA,MAAM,CAAC,YAAA,EAAc,eAAe,CAAA,GAAIA,eAAuB,MAAiB,YAAA;AAChF,EAAA,MAAM,CAAC,KAAA,EAAO,QAAQ,CAAA,GAAIA,cAAgB,EAAA;AAE1C,EAAA,MAAM,OAAU,GAAAC,iBAAA;AAAA,IACd,CAAC,CAAa,KAAA;AACZ,MAAA,eAAA,CAAgB,MAAiB,YAAA;AACjC,MAAA,WAAA,CAAY,EAAE,CAAA;AACd,MAAA,QAAA,CAAS,CAAC,CAAA;AACV,MAAA,WAAA;AAAA,QACE,+CAAA;AAAA,QACA,CAAA,6EAAA;AAAA,OACF;AACA,MAAA,OAAA,CAAQ,MAAM,CAAC,CAAA;AAAA,KACjB;AAAA,IACA,CAAC,WAAW;AAAA,GACd;AAEA,EAAA,MAAM,EAAE,KAAA,EAAO,YAAc,EAAA,KAAA,EAAO,SAAU,EAAA,GAAIC,iBAAS,CAAA,YAAY,MAAMJ,SAAA,EAAW,EAAA,CAACA,SAAO,CAAC,CAAA;AAEjG,EAAA,MAAM,EAAE,KAAO,EAAA,UAAA,EAAY,KAAM,EAAA,GAAII,kBAAS,YAAY;AACxD,IAAA,IAAI,CAAC,SAAA,IAAa,CAAC,QAAA,CAAS,MAAQ,EAAA;AAClC,MAAO,OAAA,EAAE,SAAS,SAAU,EAAA;AAAA;AAG9B,IAAA,eAAA,CAAgB,YAAuB,kBAAA;AACvC,IAAA,QAAA,CAAS,SAAS,CAAA;AAElB,IAAA,MAAM,SAAS,qBAAsB,CAAA;AAAA,MACnC,KAAA;AAAA,MACA,WAAA;AAAA,MACA;AAAA,KACD,CAAE,CAAA,IAAA;AAAA;AAAA;AAAA,MAGD,iBAAkB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,KAMpB;AAEA,IAAO,OAAA;AAAA,MACL,OAAS,EAAA,SAAA;AAAA,MACT,MAAA,EAAQ,OAAO,SAAU,CAAA;AAAA,QACvB,IAAM,EAAA,QAAA;AAAA,QACN,KAAO,EAAA,OAAA;AAAA,QACP,UAAU,MAAM;AACd,UAAA,eAAA,CAAgB,WAAsB,iBAAA;AACtC,UAAA,UAAA,CAAW,MAAM;AACf,YAAA,eAAA,CAAgB,MAAiB,YAAA;AAAA,WAClC,CAAA;AACD,UAAA,WAAA,CAAY,EAAE,CAAA;AACd,UAAA,QAAA,CAAS,SAAS,CAAA;AAAA;AACpB,OACD;AAAA,KACH;AAAA,GACC,EAAA,CAAC,QAAU,EAAA,SAAS,CAAC,CAAA;AAGxB,EAAAC,eAAA,CAAU,MAAM;AACd,IAAA,OAAO,MAAM;AACX,MAAA,IAAI,OAAO,MAAQ,EAAA;AACjB,QAAA,KAAA,CAAM,OAAO,WAAY,EAAA;AAAA;AAC3B,KACF;AAAA,GACF,EAAG,CAAC,KAAK,CAAC,CAAA;AAGV,EAAAA,eAAA,CAAU,MAAM;AACd,IAAI,IAAA,OAAA;AACJ,IAAI,IAAA,YAAA,KAAiB,YAA2B,qBAAA,KAAA,KAAU,EAAI,EAAA;AAC5D,MAAA,OAAA,GAAU,WAAW,MAAM;AACzB,QAAA,OAAA,CAAQ,IAAI,KAAA,CAAM,CAA8B,2BAAA,EAAA,OAAO,IAAI,CAAC,CAAA;AAAA,SAC3D,OAAO,CAAA;AAAA;AAEZ,IAAA,OAAO,MAAM;AACX,MAAA,OAAA,IAAW,aAAa,OAAO,CAAA;AAAA,KACjC;AAAA,GACC,EAAA,CAAC,YAAc,EAAA,KAAA,EAAO,OAAO,CAAC,CAAA;AAEjC,EAAA,IAAI,cAAc,YAAc,EAAA;AAC9B,IAAA,QAAA,CAAS,cAAc,YAAY,CAAA;AAAA;AAGrC,EAAO,OAAA;AAAA,IACL,WAAA;AAAA,IACA,KAAA;AAAA,IACA,YAAA;AAAA,IACA,KAAA;AAAA,IACA;AAAA,GACF;AACF;AAeO,SAAS,mBAGd,GAAA;AACA,EAAO,OAAAnB,SAAA;AAAA,IACLC,gBAAA,CAAO,CAAC,QAA4D,KAAA,kBAAA,CAAmB,SAAS,OAAQ,CAAA,CAAC,CAAE,CAAA,KAAK,CAAC,CAAA;AAAA;AAAA,IAEjHmB,iBAAQ,EAAA;AAAA;AAAA,IAERlB,aAAA,CAAI,CAAC,SAAoE,KAAA;AACvE,MAAM,MAAA,cAAA,GAAiB,UAAU,GAAI,CAAA,CAAA,CAAA,KAAK,EAAE,OAAQ,CAAA,CAAC,EAAE,KAAyB,CAAA;AAChF,MAAA,OAAO,uBAAuB,cAAc,CAAA;AAAA,KAC7C;AAAA,GACH;AACF;AAQO,SAAS,uBAAuB,gBAAwD,EAAA;AAC7F,EAAA,MAAM,wBAA6C,GAAA;AAAA,IACjD,IAAM,EAAA,WAAA;AAAA,IACN,YAAY;AAAC,GACf;AAEA,EAAA,KAAA,MAAW,OAAO,gBAAkB,EAAA;AAClC,IAAW,KAAA,MAAA,EAAA,IAAM,IAAI,UAAY,EAAA;AAC/B,MAAA,IAAI,EAAG,CAAA,KAAA,IAAU,wBAAyB,CAAA,UAAA,CAAW,MAAQ,EAAA;AAC3D,QAAA,wBAAA,CAAyB,WAAW,IAAK,CAAA;AAAA,UACvC,GAAG,EAAA;AAAA,UACH,QAAA,EAAU,EAAE,GAAG,EAAA,CAAG,UAAU,SAAW,EAAA,EAAA,CAAG,QAAS,CAAA,SAAA,IAAa,EAAG;AAAA,SACpE,CAAA;AAAA,OACI,MAAA;AACL,QAAyB,wBAAA,CAAA,UAAA,CAAW,GAAG,KAAM,CAAA,CAAE,SAAS,SAAa,IAAA,EAAA,CAAG,SAAS,SAAa,IAAA,EAAA;AAAA;AAChG;AACF;AAIF,EAAW,KAAA,MAAA,EAAA,IAAM,yBAAyB,UAAY,EAAA;AACpD,IAAI,IAAA,CAAC,EAAG,CAAA,QAAA,CAAS,SAAW,EAAA;AAC1B,MAAA,EAAA,CAAG,SAAS,SAAY,GAAA,IAAA;AAAA;AAC1B;AAGF,EAAO,OAAA,wBAAA;AACT;;;;;;;;;;;;;;;;;;;;;;;ACpuBO,MAAMY,YAAU,YAA8B;AACnD,EAAI,IAAA;AACF,IAAA,MAAM,WAAW,MAAMV,qBAAA,GAAgB,GAAI,CAAA,CAAA,EAAG,gBAAgB,CAAW,SAAA,CAAA,CAAA;AACzE,IAAI,IAAA,CAAC,SAAS,OAAS,EAAA;AACrB,MAAO,OAAA,KAAA;AAAA;AAGT,IAAA,MAAM,SAAS,MAAMA,qBAAA,GAAgB,GAAI,CAAA,CAAA,EAAG,gBAAgB,CAAS,OAAA,CAAA,CAAA;AACrE,IAAA,MAAM,UAAU,MAAO,CAAA,OAAA;AAGvB,IAAA,IAAI,QAAQ,WAAa,EAAA;AACvB,MAAA,OAAO,OAAQ,CAAA,WAAA,CAAY,UAAc,IAAA,OAAA,CAAQ,WAAY,CAAA,EAAA;AAAA;AAE/D,IAAA,IAAI,QAAQ,MAAQ,EAAA;AAClB,MAAA,OAAO,OAAQ,CAAA,MAAA,CAAO,UAAc,IAAA,OAAA,CAAQ,MAAO,CAAA,EAAA;AAAA;AAErD,IAAO,OAAA,KAAA;AAAA,WACA,CAAG,EAAA;AACV,IAAO,OAAA,KAAA;AAAA;AAEX,CAAA;;;;;;;;;;;;;;;;;;;;;;;AC7BA,MAAM,gBAAmB,GAAA,aAAA;AAUlB,MAAM,oBAA0C,CAAA;AAAA,EAwBrD,YAAY,IAAe,EAAA;AAvB3B,IAAA,IAAA,CAAA,eAAA,GAAkCG,yBAAkB,EAAA;AAwBlD,IAAA,IAAI,SAAS,SAAW,EAAA;AAEtB,MAAA,MAAM,SAASc,OAAK,EAAA;AACpB,MAAO,IAAA,GAAA,CAAA,EAAG,gBAAgB,CAAA,CAAA,EAAI,MAAM,CAAA,CAAA;AAAA;AAEtC,IAAA,IAAA,CAAK,iBAAoB,GAAA;AAAA,MACvB,OAAOhB,qBAAiB,CAAA,MAAA;AAAA,MACxB,SAAW,EAAA,aAAA;AAAA,MACX,IAAA,EAAM,GAAG,IAAI,CAAA,UAAA;AAAA,KACf;AACA,IAAA,IAAA,CAAK,eAAkB,GAAA;AAAA,MACrB,OAAOA,qBAAiB,CAAA,MAAA;AAAA,MACxB,SAAW,EAAA,aAAA;AAAA,MACX,IAAA,EAAM,GAAG,IAAI,CAAA,QAAA;AAAA,KACf;AAAA;AACF,EAEA,MAAM,KAAuB,GAAA;AAC3B,IAAI,IAAA,IAAA,CAAK,YAAY,SAAW,EAAA;AAC9B,MAAA,MAAM,IAAI,KAAA;AAAA,QACR;AAAA,OACF;AAAA;AAGF,IAAA,MAAM,MAAS,GAAA,IAAA,CAAK,eAAgB,CAAA,SAAA,CAAU,KAAK,iBAAiB,CAAA,CACjE,IAAK,CAAAJ,WAAA,CAAO,CAAC,KAAA,KAAUO,8BAA0B,CAAA,KAAK,CAAC,CAAC,CAAA;AAC3D,IAAA,IAAA,CAAK,OAAU,GAAA,MAAA;AACf,IAAO,MAAA,CAAA,SAAA,CAAU,CAAC,KAAU,KAAA;AAC1B,MAAI,IAAA,OAAA;AACJ,MAAI,IAAA;AACF,QAAU,OAAA,GAAAc,0BAAA,CAAqB,KAAM,CAAA,KAAA,CAAM,OAAO,CAAA;AAAA,eAC3C,KAAO,EAAA;AACd,QAAA,IAAA,CAAK,UAAU,KAAc,CAAA;AAC7B,QAAA;AAAA;AAEF,MAAA,IAAA,CAAK,YAAY,OAAO,CAAA;AAAA,KACzB,CAAA;AAAA;AACH,EAEA,MAAM,KAAK,OAAwC,EAAA;AACjD,IAAI,IAAA,IAAA,CAAK,YAAY,SAAW,EAAA;AAC9B,MAAM,MAAA,IAAI,MAAM,eAAe,CAAA;AAAA;AAkBjC,IAAA,MAAM,iBAAoB,GAAA,IAAA,CAAK,eAAgB,CAAA,OAAA,EAAS,MAAU,IAAA,CAAA;AAClE,IAAA,IAAI,iBAAmB,EAAA;AAKrB,MAAM,MAAA,OAAA,GAA8B,EAAE,SAAA,EAAW,IAAK,EAAA;AAEtD,MAAA,OAAO,KAAK,eAAgB,CAAA,OAAA,CAAQ,IAAK,CAAA,eAAA,EAAiB,SAAS,OAAO,CAAA;AAAA;AAU5E,IAAM,MAAA,sBAAA;AAAA;AAAA,MAEJ,IAAA,CAAK,eAAgB,CAAA,IAAA,EAAM,aAAe,EAAA,UAAA;AAAA,QACxC,IAAK,CAAA;AAAA,OACJ,EAAA;AAAA,KAAA;AACL,IAAA,IAAI,sBAAwB,EAAA;AAC1B,MAAO,OAAA,sBAAA,CAAuB,QAAQ,OAAO,CAAA;AAAA;AAM/C,IAAQ,OAAA,CAAA,IAAA;AAAA,MACN;AAAA,KAGF;AACA,IAAA,MAAM,IAAK,CAAA,eAAA,CAAgB,OAAQ,CAAA,IAAA,CAAK,iBAAiB,OAAO,CAAA;AAAA;AAClE,EAEA,MAAM,KAAuB,GAAA;AAC3B,IAAA,IAAA,CAAK,OAAU,GAAA,SAAA;AAAA;AAEnB;AASA,MAAM,SAAA,uBAAgB,GAAoB,EAAA;AAG1C,MAAM,gBAAA,GAAmB,KAAM,CAAA,aAAA,CAA6B,IAAI,CAAA;AAGhE,SAAS,SAAA,CAAU,SAAiB,UAAoB,EAAA;AACtD,EAAO,OAAA,CAAA,EAAG,OAAO,CAAA,CAAA,EAAI,UAAU,CAAA,CAAA;AACjC;AASA,SAAS,oBAAA,CAAqB,SAAiB,UAAoC,EAAA;AACjF,EAAA,IAAI,MAA0C,GAAA,SAAA;AAC9C,EAAA,IAAI,MAAwB,GAAA,IAAA;AAC5B,EAAA,IAAI,KAAsB,GAAA,IAAA;AAE1B,EAAM,MAAA,GAAA,GAAM,SAAU,CAAA,OAAA,EAAS,UAAU,CAAA;AACzC,EAAA,MAAM,WAAW,YAAY;AAC3B,IAAI,IAAA,SAAA,CAAU,GAAI,CAAA,GAAG,CAAG,EAAA;AACtB,MAAS,MAAA,GAAA,SAAA,CAAU,IAAI,GAAG,CAAA;AAC1B,MAAS,MAAA,GAAA,SAAA;AACT,MAAO,OAAA,MAAA;AAAA;AAGT,IAAI,IAAA;AACF,MAAM,MAAA,MAAA,GAAS,IAAIC,YAAO,CAAA;AAAA,QACxB,IAAM,EAAA,OAAA;AAAA,QACN,OAAS,EAAA;AAAA,OACV,CAAA;AACD,MAAM,MAAA,SAAA,GAAY,IAAI,oBAAqB,EAAA;AAC3C,MAAM,MAAA,MAAA,CAAO,QAAQ,SAAS,CAAA;AAC9B,MAAU,SAAA,CAAA,GAAA,CAAI,KAAK,MAAM,CAAA;AACzB,MAAS,MAAA,GAAA,SAAA;AACT,MAAS,MAAA,GAAA,MAAA;AACT,MAAO,OAAA,MAAA;AAAA,aACA,CAAG,EAAA;AACV,MAAS,MAAA,GAAA,OAAA;AACT,MAAQ,KAAA,GAAA,CAAA;AACR,MAAM,MAAA,CAAA;AAAA;AACR,GACC,GAAA;AAEH,EAAO,OAAA;AAAA,IACL,IAAO,GAAA;AACL,MAAA,IAAI,WAAW,SAAW,EAAA;AACxB,QAAM,MAAA,OAAA;AAAA,OACR,MAAA,IAAW,WAAW,OAAS,EAAA;AAC7B,QAAM,MAAA,KAAA;AAAA,OACR,MAAA,IAAW,MAAW,KAAA,SAAA,IAAa,MAAQ,EAAA;AACzC,QAAO,OAAA,MAAA;AAAA;AAET,MAAM,MAAA,IAAI,MAAM,2BAA2B,CAAA;AAAA;AAC7C,GACF;AACF;AAuCO,SAAS,iBAAkB,CAAA;AAAA,EAChC,OAAA;AAAA,EACA,UAAA;AAAA,EACA;AACF,CAA2B,EAAA;AACzB,EAAM,MAAA,QAAA,GAAW,oBAAqB,CAAA,OAAA,EAAS,UAAU,CAAA;AAKzD,EAAM,MAAA,MAAA,GAAS,SAAS,IAAK,EAAA;AAG7B,EAAA,KAAA,CAAM,UAAU,MAAM;AACpB,IAAA,OAAO,MAAM;AACX,MAAA,IAAI,MAAQ,EAAA;AACV,QAAA,MAAA,CAAO,KAAM,EAAA;AAAA;AAEf,MAAA,SAAA,CAAU,MAAO,CAAA,SAAA,CAAU,OAAS,EAAA,UAAU,CAAC,CAAA;AAAA,KACjD;AAAA,GACC,EAAA,CAAC,MAAQ,EAAA,OAAA,EAAS,UAAU,CAAC,CAAA;AAEhC,EAAA,2CACG,gBAAiB,CAAA,QAAA,EAAjB,EAA0B,KAAA,EAAO,UAC/B,QACH,CAAA;AAEJ;AASO,SAAS,YAAuB,GAAA;AACrC,EAAM,MAAA,MAAA,GAAS,KAAM,CAAA,UAAA,CAAW,gBAAgB,CAAA;AAChD,EAAA,IAAI,WAAW,IAAM,EAAA;AACnB,IAAM,MAAA,IAAI,MAAM,uDAAuD,CAAA;AAAA;AAEzE,EAAO,OAAA,MAAA;AACT;AAiBO,SAAS,qBAAqB,KAAgC,EAAA;AACnE,EAAO,OAAA,KAAA,CAAM,IAAI,mBAAmB,CAAA;AACtC;AAEA,SAAS,oBAAoB,IAA2B,EAAA;AACtD,EAAO,OAAA;AAAA,IACL,IAAM,EAAA,UAAA;AAAA,IACN,QAAU,EAAA;AAAA,MACR,MAAM,IAAK,CAAA,IAAA;AAAA,MACX,aAAa,IAAK,CAAA,WAAA;AAAA,MAClB,YAAY,IAAK,CAAA,WAAA,CAAY,UAAe,KAAA,SAAA,GAAY,KAAK,WAAc,GAAA;AAAA;AAC7E,GACF;AACF;;;;;;;;;;;AClQA,eAAsB,OAAsC,OAAyD,EAAA;AACnH,EAAM,MAAA,QAAA,GAAW,MAAMnB,qBAAA,EAAgB,CAAA,IAAA;AAAA,IACrC,sDAAA;AAAA,IACA,OAAA;AAAA,IACA;AAAA,MACE,OAAA,EAAS,EAAE,cAAA,EAAgB,kBAAmB;AAAA;AAChD,GACF;AACA,EAAA,OAAO,QAAS,CAAA,OAAA;AAClB;AAEA,IAAI,aAAgB,GAAA,KAAA;AAGb,MAAM,SAAS,YAA0C;AAE9D,EAAI,IAAA;AACF,IAAM,MAAA,QAAA,GAAW,MAAMA,qBAAc,EAAA,CAAE,IAAI,CAAG,EAAA,gBAAgB,CAAa,SAAA,CAAA,EAAA,KAAA,CAAA,EAAW,KAAW,CAAA,EAAA;AAAA,MAC/F,gBAAkB,EAAA,KAAA;AAAA,MAClB,cAAgB,EAAA;AAAA,KACjB,CAAA;AACD,IAAI,IAAA,CAAC,SAAS,OAAS,EAAA;AACrB,MAAA,OAAO,EAAE,OAAS,EAAA,KAAA,EAAO,EAAI,EAAA,KAAA,EAAO,OAAO,wCAAyC,EAAA;AAAA;AACtF,WACO,CAAG,EAAA;AACV,IAASS,gBAAA,CAAA,MAAA,CAAO,CAAC,CAAC,CAAA;AAClB,IAAAA,gBAAA;AAAA,MACE;AAAA,KACF;AACA,IAAgB,aAAA,GAAA,IAAA;AAChB,IAAA,OAAO,EAAE,OAAS,EAAA,KAAA,EAAO,EAAI,EAAA,KAAA,EAAO,OAAO,0CAA2C,EAAA;AAAA;AAIxF,EAAI,IAAA,QAAA;AACJ,EAAI,IAAA;AACF,IAAW,QAAA,GAAA,MAAMT,uBAAgB,CAAA,GAAA,CAAI,GAAG,gBAAgB,CAAA,OAAA,CAAA,EAAW,QAAW,KAAW,CAAA,EAAA;AAAA,MACvF,gBAAkB,EAAA,KAAA;AAAA,MAClB,cAAgB,EAAA;AAAA,KACjB,CAAA;AAAA,WACM,CAAG,EAAA;AAGV,IAAA,IAAI,CAAC,aAAe,EAAA;AAClB,MAASS,gBAAA,CAAA,MAAA,CAAO,CAAC,CAAC,CAAA;AAClB,MAAAA,gBAAA;AAAA,QACE;AAAA,OACF;AACA,MAAgB,aAAA,GAAA,IAAA;AAAA;AAElB,IAAA,OAAO,EAAE,OAAS,EAAA,KAAA,EAAO,EAAI,EAAA,KAAA,EAAO,OAAO,0CAA2C,EAAA;AAAA;AAGxF,EAAM,MAAA,EAAE,SAAY,GAAA,QAAA;AAEpB,EAAI,IAAA,OAAA,EAAS,YAAY,SAAW,EAAA;AAClC,IAAA,mBAAA,CAAoB,QAAQ,OAAO,CAAA;AAAA;AAErC,EAAI,IAAA,OAAA,EAAS,WAAW,SAAW,EAAA;AACjC,IAAA,OAAO,EAAE,OAAS,EAAA,KAAA,EAAO,EAAI,EAAA,KAAA,EAAO,OAAO,uDAAwD,EAAA;AAAA;AAErG,EAAA,OAAO,OAAO,OAAA,CAAQ,MAAW,KAAA,SAAA,GAAY,EAAE,OAAA,EAAS,OAAQ,CAAA,MAAA,EAAQ,EAAI,EAAA,OAAA,CAAQ,MAAO,EAAA,GAAI,OAAQ,CAAA,MAAA;AACzG,CAAA;AAEO,MAAM,UAAU,YAA8B;AACnD,EAAM,MAAA,aAAA,GAAgB,MAAM,MAAO,EAAA;AACnC,EAAO,OAAA,aAAA,CAAc,WAAW,aAAc,CAAA,EAAA;AAChD,CAAA;;;;;;;;;;;;;;"}