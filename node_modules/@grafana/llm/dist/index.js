'use strict';

var runtime = require('@grafana/runtime');
var semver = require('semver');
var data = require('@grafana/data');
var React = require('react');
var reactUse = require('react-use');
var rxjs = require('rxjs');
var operators = require('rxjs/operators');
var uuid = require('uuid');
var index = require('@modelcontextprotocol/sdk/client/index');
var types = require('@modelcontextprotocol/sdk/types');

const LLM_PLUGIN_ID = "grafana-llm-app";
const LLM_PLUGIN_ROUTE = `/api/plugins/${LLM_PLUGIN_ID}`;
let LLM_PLUGIN_VERSION = new semver.SemVer("0.2.0");
function setLLMPluginVersion(version) {
  try {
    LLM_PLUGIN_VERSION = new semver.SemVer(version);
  } catch (e) {
    runtime.logWarning("Failed to parse version of grafana-llm-app; assuming old version is present.");
  }
}

const LLM_CHAT_COMPLETIONS_PATH = "llm/v1/chat/completions";
var Model = /* @__PURE__ */ ((Model2) => {
  Model2["BASE"] = "base";
  Model2["LARGE"] = "large";
  return Model2;
})(Model || {});
function isContentMessage(message) {
  return "content" in message;
}
function isDoneMessage(message) {
  return "done" in message && message.done != null;
}
function isErrorResponse(response) {
  return "error" in response;
}
function isFunctionCallMessage(message) {
  return "name" in message && "arguments" in message;
}
function isToolCallsMessage(message) {
  return "tool_calls" in message && message.tool_calls != null;
}
function extractContent() {
  return rxjs.pipe(
    operators.filter((response) => isContentMessage(response.choices[0].delta)),
    // The type assertion is needed here because the type predicate above doesn't seem to propagate.
    operators.map(
      (response) => response.choices[0].delta.content
    )
  );
}
function accumulateContent() {
  return rxjs.pipe(
    extractContent(),
    operators.scan((acc, curr) => acc + curr, "")
  );
}
async function chatCompletions(request) {
  const response = await runtime.getBackendSrv().post(
    `/api/plugins/grafana-llm-app/resources/${LLM_CHAT_COMPLETIONS_PATH}`,
    request,
    {
      headers: { "Content-Type": "application/json" }
    }
  );
  return response;
}
function streamChatCompletions(request) {
  const channel = {
    scope: data.LiveChannelScope.Plugin,
    namespace: LLM_PLUGIN_ID,
    path: LLM_CHAT_COMPLETIONS_PATH + "/" + uuid.v4(),
    data: request
  };
  const messages = runtime.getGrafanaLiveSrv().getStream(channel).pipe(operators.filter((event) => data.isLiveChannelMessageEvent(event)));
  return messages.pipe(
    // Filter out messages that don't have the expected structure
    operators.filter((event) => {
      if (!event.message.choices) {
        return false;
      }
      return true;
    }),
    operators.tap((event) => {
      if (isErrorResponse(event.message)) {
        throw new Error(event.message.error);
      }
    }),
    // Stop the stream when we get a done message or when the finish_reason is "stop"
    operators.takeWhile((event) => {
      if (isErrorResponse(event.message)) {
        return true;
      }
      if (event.message.choices && event.message.choices[0].delta && "done" in event.message.choices[0].delta && event.message.choices[0].delta.done === true) {
        return false;
      }
      if (event.message.choices && "finish_reason" in event.message.choices[0] && event.message.choices[0].finish_reason === "stop") {
        return false;
      }
      return true;
    }),
    operators.map((event) => event.message)
  );
}
let loggedWarning$1 = false;
const health$1 = async () => {
  try {
    const settings = await runtime.getBackendSrv().get(`${LLM_PLUGIN_ROUTE}/settings`, void 0, void 0, {
      showSuccessAlert: false,
      showErrorAlert: false
    });
    if (!settings.enabled) {
      return { configured: false, ok: false, error: "The Grafana LLM plugin is not enabled." };
    }
  } catch (e) {
    runtime.logDebug(String(e));
    runtime.logDebug(
      "Failed to check if LLM provider is enabled. This is expected if the Grafana LLM plugin is not installed, and the above error can be ignored."
    );
    loggedWarning$1 = true;
    return { configured: false, ok: false, error: "The Grafana LLM plugin is not installed." };
  }
  let response;
  try {
    response = await runtime.getBackendSrv().get(`${LLM_PLUGIN_ROUTE}/health`, void 0, void 0, {
      showSuccessAlert: false,
      showErrorAlert: false
    });
  } catch (e) {
    if (!loggedWarning$1) {
      runtime.logDebug(String(e));
      runtime.logDebug(
        "Failed to check if LLM provider is enabled. This is expected if the Grafana LLM plugin is not installed, and the above error can be ignored."
      );
      loggedWarning$1 = true;
    }
    return { configured: false, ok: false, error: "The Grafana LLM plugin is not installed." };
  }
  const { details } = response;
  if (details?.version !== undefined) {
    setLLMPluginVersion(details.version);
  }
  if (details?.llmProvider === undefined) {
    return { configured: false, ok: false, error: "The Grafana LLM plugin is outdated; please update it." };
  }
  return typeof details.llmProvider === "boolean" ? { configured: details.llmProvider, ok: details.llmProvider } : details.llmProvider;
};
const enabled$2 = async () => {
  const healthDetails = await health$1();
  return healthDetails.configured && healthDetails.ok;
};
var StreamStatus = /* @__PURE__ */ ((StreamStatus2) => {
  StreamStatus2["IDLE"] = "idle";
  StreamStatus2["GENERATING"] = "generating";
  StreamStatus2["COMPLETED"] = "completed";
  return StreamStatus2;
})(StreamStatus || {});
const TIMEOUT = 1e4;
function useLLMStream(model = "large" /* LARGE */, temperature = 1, notifyError = () => {
}) {
  const [messages, setMessages] = React.useState([]);
  const [reply, setReply] = React.useState("");
  const [streamStatus, setStreamStatus] = React.useState("idle" /* IDLE */);
  const [error, setError] = React.useState();
  const onError = React.useCallback(
    (e) => {
      setStreamStatus("idle" /* IDLE */);
      setMessages([]);
      setError(e);
      notifyError(
        "Failed to generate content using LLM provider",
        `Please try again or if the problem persists, contact your organization admin.`
      );
      console.error(e);
    },
    [notifyError]
  );
  const { error: enabledError, value: isEnabled } = reactUse.useAsync(async () => await enabled$2(), [enabled$2]);
  const { error: asyncError, value } = reactUse.useAsync(async () => {
    if (!isEnabled || !messages.length) {
      return { enabled: isEnabled };
    }
    setStreamStatus("generating" /* GENERATING */);
    setError(undefined);
    const stream = streamChatCompletions({
      model,
      temperature,
      messages
    }).pipe(
      // Accumulate the stream content into a stream of strings, where each
      // element contains the accumulated message so far.
      accumulateContent()
      // The stream is just a regular Observable, so we can use standard rxjs
      // functionality to update state, e.g. recording when the stream
      // has completed.
      // The operator decision tree on the rxjs website is a useful resource:
      // https://rxjs.dev/operator-decision-tree.)
    );
    return {
      enabled: isEnabled,
      stream: stream.subscribe({
        next: setReply,
        error: onError,
        complete: () => {
          setStreamStatus("completed" /* COMPLETED */);
          setTimeout(() => {
            setStreamStatus("idle" /* IDLE */);
          });
          setMessages([]);
          setError(undefined);
        }
      })
    };
  }, [messages, isEnabled]);
  React.useEffect(() => {
    return () => {
      if (value?.stream) {
        value.stream.unsubscribe();
      }
    };
  }, [value]);
  React.useEffect(() => {
    let timeout;
    if (streamStatus === "generating" /* GENERATING */ && reply === "") {
      timeout = setTimeout(() => {
        onError(new Error(`LLM stream timed out after ${TIMEOUT}ms`));
      }, TIMEOUT);
    }
    return () => {
      timeout && clearTimeout(timeout);
    };
  }, [streamStatus, reply, onError]);
  if (asyncError || enabledError) {
    setError(asyncError || enabledError);
  }
  return {
    setMessages,
    reply,
    streamStatus,
    error,
    value
  };
}
function accumulateToolCalls() {
  return rxjs.pipe(
    operators.filter((response) => isToolCallsMessage(response.choices[0].delta)),
    // Collect all tool call chunks
    operators.toArray(),
    // Process the array to reconstruct the complete tool call message
    operators.map((responses) => {
      const toolCallChunks = responses.map((r) => r.choices[0].delta);
      return recoverToolCallMessage(toolCallChunks);
    })
  );
}
function recoverToolCallMessage(toolCallMessages) {
  const recoveredToolCallMessage = {
    role: "assistant",
    tool_calls: []
  };
  for (const msg of toolCallMessages) {
    for (const tc of msg.tool_calls) {
      if (tc.index >= recoveredToolCallMessage.tool_calls.length) {
        recoveredToolCallMessage.tool_calls.push({
          ...tc,
          function: { ...tc.function, arguments: tc.function.arguments ?? "" }
        });
      } else {
        recoveredToolCallMessage.tool_calls[tc.index].function.arguments += tc.function.arguments ?? "";
      }
    }
  }
  for (const tc of recoveredToolCallMessage.tool_calls) {
    if (!tc.function.arguments) {
      tc.function.arguments = "{}";
    }
  }
  return recoveredToolCallMessage;
}

var llm = /*#__PURE__*/Object.freeze({
  __proto__: null,
  Model: Model,
  StreamStatus: StreamStatus,
  TIMEOUT: TIMEOUT,
  accumulateContent: accumulateContent,
  accumulateToolCalls: accumulateToolCalls,
  chatCompletions: chatCompletions,
  enabled: enabled$2,
  extractContent: extractContent,
  health: health$1,
  isContentMessage: isContentMessage,
  isDoneMessage: isDoneMessage,
  isErrorResponse: isErrorResponse,
  isFunctionCallMessage: isFunctionCallMessage,
  isToolCallsMessage: isToolCallsMessage,
  recoverToolCallMessage: recoverToolCallMessage,
  streamChatCompletions: streamChatCompletions,
  useLLMStream: useLLMStream
});

const enabled$1 = async () => {
  try {
    const settings = await runtime.getBackendSrv().get(`${LLM_PLUGIN_ROUTE}/settings`);
    if (!settings.enabled) {
      return false;
    }
    const health = await runtime.getBackendSrv().get(`${LLM_PLUGIN_ROUTE}/health`);
    const details = health.details;
    if (details.llmProvider) {
      return details.llmProvider.configured && details.llmProvider.ok;
    }
    if (details.openAI) {
      return details.openAI.configured && details.openAI.ok;
    }
    return false;
  } catch (e) {
    return false;
  }
};

var openai = /*#__PURE__*/Object.freeze({
  __proto__: null,
  Model: Model,
  StreamStatus: StreamStatus,
  TIMEOUT: TIMEOUT,
  accumulateContent: accumulateContent,
  accumulateToolCalls: accumulateToolCalls,
  chatCompletions: chatCompletions,
  enabled: enabled$1,
  extractContent: extractContent,
  health: health$1,
  isContentMessage: isContentMessage,
  isDoneMessage: isDoneMessage,
  isErrorResponse: isErrorResponse,
  isFunctionCallMessage: isFunctionCallMessage,
  isToolCallsMessage: isToolCallsMessage,
  recoverToolCallMessage: recoverToolCallMessage,
  streamChatCompletions: streamChatCompletions,
  useLLMStream: useLLMStream
});

const MCP_GRAFANA_PATH = "mcp/grafana";
class GrafanaLiveTransport {
  constructor(path) {
    this._grafanaLiveSrv = runtime.getGrafanaLiveSrv();
    if (path === undefined) {
      const pathId = uuid.v4();
      path = `${MCP_GRAFANA_PATH}/${pathId}`;
    }
    this._subscribeChannel = {
      scope: data.LiveChannelScope.Plugin,
      namespace: LLM_PLUGIN_ID,
      path: `${path}/subscribe`
    };
    this._publishChannel = {
      scope: data.LiveChannelScope.Plugin,
      namespace: LLM_PLUGIN_ID,
      path: `${path}/publish`
    };
  }
  async start() {
    if (this._stream !== undefined) {
      throw new Error(
        "GrafanaLiveTransport already started! If using Client class, note that connect() calls start() automatically."
      );
    }
    const stream = this._grafanaLiveSrv.getStream(this._subscribeChannel).pipe(rxjs.filter((event) => data.isLiveChannelMessageEvent(event)));
    this._stream = stream;
    stream.subscribe((event) => {
      let message;
      try {
        message = types.JSONRPCMessageSchema.parse(event.message);
      } catch (error) {
        this.onerror?.(error);
        return;
      }
      this.onmessage?.(message);
    });
  }
  async send(message) {
    if (this._stream === undefined) {
      throw new Error("not connected");
    }
    const hasPublishOptions = this._grafanaLiveSrv.publish?.length >= 3;
    if (hasPublishOptions) {
      const options = { useSocket: true };
      return this._grafanaLiveSrv.publish(this._publishChannel, message, options);
    }
    const centrifugeSubscription = (
      // @ts-expect-error
      this._grafanaLiveSrv.deps?.centrifugeSrv?.getChannel?.(
        this._publishChannel
      )?.subscription
    );
    if (centrifugeSubscription) {
      return centrifugeSubscription.publish(message);
    }
    console.warn(
      "Websocket subscription not available, falling back to HTTP publish. This may fail in HA setups. If you see this, please create an issue at https://github.com/grafana/grafana-llm-app/issues/new."
    );
    await this._grafanaLiveSrv.publish(this._publishChannel, message);
  }
  async close() {
    this._stream = undefined;
  }
}
const clientMap = /* @__PURE__ */ new Map();
const MCPClientContext = React.createContext(null);
function clientKey(appName, appVersion) {
  return `${appName}-${appVersion}`;
}
function createClientResource(appName, appVersion) {
  let status = "pending";
  let result = null;
  let error = null;
  const key = clientKey(appName, appVersion);
  const promise = (async () => {
    if (clientMap.has(key)) {
      result = clientMap.get(key);
      status = "success";
      return result;
    }
    try {
      const client = new index.Client({
        name: appName,
        version: appVersion
      });
      const transport = new GrafanaLiveTransport();
      await client.connect(transport);
      clientMap.set(key, client);
      status = "success";
      result = client;
      return client;
    } catch (e) {
      status = "error";
      error = e;
      throw e;
    }
  })();
  return {
    read() {
      if (status === "pending") {
        throw promise;
      } else if (status === "error") {
        throw error;
      } else if (status === "success" && result) {
        return result;
      }
      throw new Error("Unexpected resource state");
    }
  };
}
function MCPClientProvider({
  appName,
  appVersion,
  children
}) {
  const resource = createClientResource(appName, appVersion);
  const client = resource.read();
  React.useEffect(() => {
    return () => {
      if (client) {
        client.close();
      }
      clientMap.delete(clientKey(appName, appVersion));
    };
  }, [client, appName, appVersion]);
  return /* @__PURE__ */ React.createElement(MCPClientContext.Provider, { value: client }, children);
}
function useMCPClient() {
  const client = React.useContext(MCPClientContext);
  if (client === null) {
    throw new Error("useMCPClient must be used within an MCPClientProvider");
  }
  return client;
}
function convertToolsToOpenAI(tools) {
  return tools.map(convertToolToOpenAI);
}
function convertToolToOpenAI(tool) {
  return {
    type: "function",
    function: {
      name: tool.name,
      description: tool.description,
      parameters: tool.inputSchema.properties !== undefined ? tool.inputSchema : undefined
    }
  };
}

var mcp = /*#__PURE__*/Object.freeze({
  __proto__: null,
  Client: index.Client,
  GrafanaLiveTransport: GrafanaLiveTransport,
  MCPClientProvider: MCPClientProvider,
  convertToolsToOpenAI: convertToolsToOpenAI,
  useMCPClient: useMCPClient
});

async function search(request) {
  const response = await runtime.getBackendSrv().post(
    "/api/plugins/grafana-llm-app/resources/vector/search",
    request,
    {
      headers: { "Content-Type": "application/json" }
    }
  );
  return response.results;
}
let loggedWarning = false;
const health = async () => {
  try {
    const settings = await runtime.getBackendSrv().get(`${LLM_PLUGIN_ROUTE}/settings`, void 0, void 0, {
      showSuccessAlert: false,
      showErrorAlert: false
    });
    if (!settings.enabled) {
      return { enabled: false, ok: false, error: "The Grafana LLM plugin is not enabled." };
    }
  } catch (e) {
    runtime.logDebug(String(e));
    runtime.logDebug(
      "Failed to check if the vector service is enabled. This is expected if the Grafana LLM plugin is not installed, and the above error can be ignored."
    );
    loggedWarning = true;
    return { enabled: false, ok: false, error: "The Grafana LLM plugin is not installed." };
  }
  let response;
  try {
    response = await runtime.getBackendSrv().get(`${LLM_PLUGIN_ROUTE}/health`, void 0, void 0, {
      showSuccessAlert: false,
      showErrorAlert: false
    });
  } catch (e) {
    if (!loggedWarning) {
      runtime.logDebug(String(e));
      runtime.logDebug(
        "Failed to check if vector service is enabled. This is expected if the Grafana LLM plugin is not installed, and the above error can be ignored."
      );
      loggedWarning = true;
    }
    return { enabled: false, ok: false, error: "The Grafana LLM plugin is not installed." };
  }
  const { details } = response;
  if (details?.version !== undefined) {
    setLLMPluginVersion(details.version);
  }
  if (details?.vector === undefined) {
    return { enabled: false, ok: false, error: "The Grafana LLM plugin is outdated; please update it." };
  }
  return typeof details.vector === "boolean" ? { enabled: details.vector, ok: details.vector } : details.vector;
};
const enabled = async () => {
  const healthDetails = await health();
  return healthDetails.enabled && healthDetails.ok;
};

var vector = /*#__PURE__*/Object.freeze({
  __proto__: null,
  enabled: enabled,
  health: health,
  search: search
});

exports.llm = llm;
exports.mcp = mcp;
exports.openai = openai;
exports.vector = vector;
//# sourceMappingURL=index.js.map
