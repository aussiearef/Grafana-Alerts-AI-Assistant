import React from 'react';
import { UnaryFunction, Observable, Subscription } from 'rxjs';
import { LiveChannelAddress, LiveChannelMessageEvent } from '@grafana/data';
import { GrafanaLiveSrv } from '@grafana/runtime';
import { Transport } from '@modelcontextprotocol/sdk/shared/transport';
import { Client } from '@modelcontextprotocol/sdk/client/index';
import { JSONRPCMessage, Tool as Tool$1 } from '@modelcontextprotocol/sdk/types';

interface LLMProviderHealthDetails {
    configured: boolean;
    ok: boolean;
    error?: string;
    models?: Record<string, ModelHealthDetails>;
    assistant?: ModelHealthDetails;
}
interface ModelHealthDetails {
    ok: boolean;
    error?: string;
}
interface VectorHealthDetails {
    enabled: boolean;
    ok: boolean;
    error?: string;
}

/**
 * LLM API client.
 *
 * This module contains functions used to make requests to the LLM provider API via
 * the Grafana LLM app plugin. That plugin must be installed, enabled and configured
 * in order for these functions to work.
 *
 * The {@link enabled} function can be used to check if the plugin is enabled and configured.
 */

/** The role of a message's author. */
type Role = 'system' | 'user' | 'assistant' | 'function' | 'tool';
/** A message in a conversation. */
interface Message {
    /** The role of the message's author. */
    role: Role;
    /** The contents of the message. content is required for all messages, and may be null for assistant messages with function calls. */
    content?: string;
    /** The ID of the tool call, if this message is a function call. */
    tool_call_id?: string;
    /**
     * The name of the author of this message.
     *
     * This is required if role is 'function', and it should be the name of the function whose response is in the content.
     *
     * May contain a-z, A-Z, 0-9, and underscores, with a maximum length of 64 characters.
     */
    name?: string;
    /**
     * The name and arguments of a function that should be called, as generated by the model.
     *
     * @deprecated Use tool_calls instead.
     */
    function_call?: Object;
    /**
     * The tool calls generated by the model, such as function calls.
     */
    tool_calls?: ToolCall[];
}
/** A tool call the model may generate. */
interface ToolCall {
    id: string;
    index?: number;
    type: 'function';
    function: FunctionCall;
}
/** A function call generated by the model. */
interface FunctionCall {
    /**
     * The name of the tool to call.
     */
    name: string;
    /**
     * The arguments to call the function with, as generated by the model in JSON format.
     *
     * Note that the model does not always generate valid JSON, and may hallucinate
     * parameters not defined by your function schema. Validate the arguments in
     * your code before calling your function.
     */
    arguments: string;
}
/** A function the model may generate JSON inputs for. */
interface Function {
    /**
     * The name of the function to be called.
     *
     * Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.
     */
    name: string;
    /**
     * A description of what the function does, used by the model to choose when and how to call the function.
     */
    description?: string;
    parameters?: Object;
    /**
     * Whether to enable strict schema adherence when generating the function call.
     *
     * If set to true, the model will follow the exact schema defined in the parameters field.
     * Only a subset of JSON Schema is supported when strict is true.
     */
    strict?: boolean;
}
/**
 * Enum representing abstracted models used by the backend app.
 * @enum {string}
 */
declare enum Model {
    BASE = "base",
    LARGE = "large"
}
/**
 * @deprecated Use {@link Model} instead.
 */
type DeprecatedString = string;
interface ChatCompletionsRequest {
    /**
     * Model abstraction to use. These abstractions are then translated back into specific models based on the users settings.
     *
     * If not specified, defaults to `Model.BASE`.
     */
    model?: Model | DeprecatedString;
    /** A list of messages comprising the conversation so far. */
    messages: Message[];
    /**
     * What sampling temperature to use, between 0 and 2.
     * Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
     *
     * We generally recommend altering this or top_p but not both.
     */
    temperature?: number;
    /**
     * An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.
     * So 0.1 means only the tokens comprising the top 10% probability mass are considered.
     *
     * We generally recommend altering this or temperature but not both.
     */
    top_p?: number;
    /**
     * How many chat completion choices to generate for each input message.
     */
    n?: number;
    /**
     * Up to 4 sequences where the API will stop generating further tokens.
     */
    stop?: string | string[];
    /**
     * The maximum number of tokens to generate in the chat completion.
     *
     * The total length of input tokens and generated tokens is limited by the model's context length. Example Python code for counting tokens.
     */
    max_tokens?: number;
    /**
     * Number between -2.0 and 2.0.
     *
     * Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
     */
    presence_penalty?: number;
    /**
     * Number between -2.0 and 2.0.
     *
     * Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
     */
    frequency_penalty?: number;
    /**
     * Modify the likelihood of specified tokens appearing in the completion.
     *
     * Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100.
     * Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model,
     * but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban
     * or exclusive selection of the relevant token.
     */
    logit_bias?: {
        [key: string]: number;
    };
    /**
     * A unique identifier representing your end-user, which can help monitor and detect abuse.
     */
    user?: string;
    /** A list of tools that the model may use. */
    tools?: Tool[];
}
/** A tool that the model may use. */
interface Tool {
    type: 'function';
    /** The function that the model may use. */
    function: Function;
}
/** A completion object from the LLM provider. */
interface Choice {
    /** The message object generated by the model. */
    message: Message;
    /**
     * The reason the model stopped generating text.
     *
     * This may be one of:
     *  - stop: API returned complete message, or a message terminated by one of the stop sequences provided via the stop parameter
     *  - length: incomplete model output due to max_tokens parameter or token limit
     *  - function_call: the model decided to call a function
     *  - content_filter: omitted content due to a flag from our content filters
     *  - null: API response still in progress or incomplete
     */
    finish_reason: string;
    /** The index of the completion in the list of choices. */
    index: number;
}
/** The usage statistics for a request to the LLM provider. */
interface Usage {
    /** The number of tokens in the prompt. */
    prompt_tokens: number;
    /** The number of tokens in the completion. */
    completion_tokens: number;
    /** The total number of tokens. */
    total_tokens: number;
}
/** The error response from the Grafana LLM app when trying to call the chat completions API. */
interface ChatCompletionsErrorResponse {
    /** The error message. */
    error: string;
}
/** A response from the LLM provider Chat Completions API. */
interface ChatCompletionsResponse<T = Choice> {
    /** The ID of the request. */
    id: string;
    /** The type of object returned (e.g. 'chat.completion'). */
    object: string;
    /** The timestamp of the request, as a UNIX timestamp. */
    created: number;
    /** The name of the model used to generate the response. */
    model: string;
    /** A list of completion objects (only one, unless `n > 1` in the request). */
    choices: T[];
    /** The number of tokens used to generate the replies, counting prompt, completion, and total. */
    usage: Usage;
}
/** A content message returned from the model. */
interface ContentMessage {
    /** The content of the message. */
    content: string;
    /** The role of the author of this message. */
    role: Role;
}
/** A message returned from the model indicating that it is done. */
interface DoneMessage {
    done: boolean;
}
/** A function call message returned from the model. */
interface FunctionCallMessage {
    /** The name of the function to call. */
    name: string;
    /** The arguments to the function call. */
    arguments: any[];
}
/** A tool calls message returned from the model. */
interface ToolCallsMessage {
    /** The tool calls generated by the model. */
    tool_calls: ToolCall[];
    /** The role of the author of this message. */
    role: Role;
}
/**
 * A delta returned from a stream of chat completion responses.
 *
 * In practice this will be either a content message or a function call;
 * done messages are filtered out by the `streamChatCompletions` function.
 */
type ChatCompletionsDelta = ContentMessage | FunctionCallMessage | DoneMessage | ToolCallsMessage;
/** A chunk included in a chat completion response. */
interface ChatCompletionsChunk {
    /** The delta since the previous chunk. */
    delta: ChatCompletionsDelta;
}
/** Return true if the message is a 'content' message. */
declare function isContentMessage(message: ChatCompletionsDelta): message is ContentMessage;
/** Return true if the message is a 'done' message. */
declare function isDoneMessage(message: ChatCompletionsDelta): message is DoneMessage;
/** Return true if the response is an error response. */
declare function isErrorResponse<T>(response: ChatCompletionsResponse<T> | ChatCompletionsErrorResponse): response is ChatCompletionsErrorResponse;
/** Return true if the message is a function call message. */
declare function isFunctionCallMessage(message: ChatCompletionsDelta): message is FunctionCallMessage;
/** Return true if the message is a tool calls message. */
declare function isToolCallsMessage(message: ChatCompletionsDelta): message is ToolCallsMessage;
/**
 * An rxjs operator that extracts the content messages from a stream of chat completion responses.
 *
 * @returns An observable that emits the content messages. Each emission will be a string containing the
 *         token emitted by the model.
 * @example <caption>Example of reading all tokens in a stream.</caption>
 * const stream = streamChatCompletions({ model: Model.BASE, messages: [
 *   { role: 'system', content: 'You are a great bot.' },
 *   { role: 'user', content: 'Hello, bot.' },
 * ]}).pipe(extractContent());
 * stream.subscribe({ next: console.log, error: console.error });
 * // Output:
 * // ['Hello', '? ', 'How ', 'are ', 'you', '?']
 */
declare function extractContent(): UnaryFunction<Observable<ChatCompletionsResponse<ChatCompletionsChunk>>, Observable<string>>;
/**
 * An rxjs operator that accumulates the content messages from a stream of chat completion responses.
 *
 * @returns An observable that emits the accumulated content messages. Each emission will be a string containing the
 *         content of all messages received so far.
 * @example
 * const stream = streamChatCompletions({ model: Model.BASE, messages: [
 *   { role: 'system', content: 'You are a great bot.' },
 *   { role: 'user', content: 'Hello, bot.' },
 * ]}).pipe(accumulateContent());
 * stream.subscribe({ next: console.log, error: console.error });
 * // Output:
 * // ['Hello', 'Hello! ', 'Hello! How ', 'Hello! How are ', 'Hello! How are you', 'Hello! How are you?']
 */
declare function accumulateContent(): UnaryFunction<Observable<ChatCompletionsResponse<ChatCompletionsChunk>>, Observable<string>>;
/**
 * Make a request to the chat-completions API via the Grafana LLM plugin proxy.
 */
declare function chatCompletions(request: ChatCompletionsRequest): Promise<ChatCompletionsResponse>;
/**
 * Make a streaming request to the chat-completions API via the Grafana LLM plugin proxy.
 *
 * A stream of tokens will be returned as an `Observable<string>`. Use the `extractContent` operator to
 * filter the stream to only content messages, or the `accumulateContent` operator to obtain a stream of
 * accumulated content messages.
 *
 * The 'done' message will not be emitted; the stream will simply end when this message is encountered.
 *
 * @example <caption>Example of reading all tokens in a stream.</caption>
 * const stream = streamChatCompletions({ model: Model.BASE, messages: [
 *   { role: 'system', content: 'You are a great bot.' },
 *   { role: 'user', content: 'Hello, bot.' },
 * ]}).pipe(extractContent());
 * stream.subscribe({ next: console.log, error: console.error });
 * // Output:
 * // ['Hello', '? ', 'How ', 'are ', 'you', '?']
 *
 * @example <caption>Example of accumulating tokens in a stream.</caption>
 * const stream = streamChatCompletions({ model: Model.BASE, messages: [
 *   { role: 'system', content: 'You are a great bot.' },
 *   { role: 'user', content: 'Hello, bot.' },
 * ]}).pipe(accumulateContent());
 * stream.subscribe({ next: console.log, error: console.error });
 * // Output:
 * // ['Hello', 'Hello! ', 'Hello! How ', 'Hello! How are ', 'Hello! How are you', 'Hello! How are you?']
 */
declare function streamChatCompletions(request: ChatCompletionsRequest): Observable<ChatCompletionsResponse<ChatCompletionsChunk>>;
/** Check if the LLM provider API is enabled via the LLM plugin. */
declare const health$1: () => Promise<LLMProviderHealthDetails>;
declare const enabled$2: () => Promise<boolean>;
/**
 * Enum representing different states for a stream.
 * @enum {string}
 */
declare enum StreamStatus {
    IDLE = "idle",
    GENERATING = "generating",
    COMPLETED = "completed"
}
/**
 * A constant representing the timeout value in milliseconds.
 * @type {number}
 */
declare const TIMEOUT = 10000;
/**
 * A type representing the state of an LLM stream.
 * @typedef {Object} LLMStreamState
 * @property {React.Dispatch<React.SetStateAction<Message[]>} setMessages - A function to set messages.
 * @property {string} reply - The reply associated with the stream.
 * @property {typeof StreamStatus} streamStatus - The current status of the stream.
 * @property {Error|undefined} error - An optional error associated with the stream.
 * @property {{
 *    enabled: boolean|undefined;
 *    stream?: undefined;
 *  }|{
 *    enabled: boolean|undefined;
 *    stream: Subscription;
 *  }|undefined} value - A value that can be an object with 'enabled' and 'stream' properties or undefined.
 */
type LLMStreamState = {
    setMessages: React.Dispatch<React.SetStateAction<Message[]>>;
    reply: string;
    streamStatus: StreamStatus;
    error: Error | undefined;
    value: {
        enabled: boolean | undefined;
        stream?: undefined;
    } | {
        enabled: boolean | undefined;
        stream: Subscription;
    } | undefined;
};
/**
 * A custom React hook for managing an LLM stream that communicates with the provided model.
 *
 * @param {string} [model=Model.LARGE] - The LLM model to use for communication.
 * @param {number} [temperature=1] - The temperature value for text generation (default is 1).
 * @param {function} [notifyError] - A callback function for handling errors.
 *
 * @returns {LLMStreamState} - An object containing the state of the LLM stream.
 * @property {function} setMessages - A function to update the list of messages in the stream.
 * @property {string} reply - The most recent reply received from the LLM stream.
 * @property {StreamStatus} streamStatus - The status of the stream ("idle", "generating" or "completed").
 * @property {Error|undefined} error - An error object if an error occurs, or undefined if no error.
 * @property {object|undefined} value - The current value of the stream.
 * @property {boolean|undefined} value.enabled - Indicates whether the stream is enabled (true or false).
 * @property {Subscription|undefined} value.stream - The stream subscription object if the stream is active, or undefined if not.
 */
declare function useLLMStream(model?: Model, temperature?: number, notifyError?: (title: string, text?: string, traceId?: string) => void): LLMStreamState;
/**
 * An rxjs operator that accumulates tool call messages from a stream of chat completion responses into a complete tool call message.
 *
 * @returns An observable that emits the accumulated tool call message when complete.
 * @example
 * const stream = streamChatCompletions({...}).pipe(
 *   accumulateToolCalls()
 * );
 * stream.subscribe({
 *   next: (toolCallMessage) => console.log('Received complete tool call:', toolCallMessage),
 *   error: console.error
 * });
 */
declare function accumulateToolCalls(): UnaryFunction<Observable<ChatCompletionsResponse<ChatCompletionsChunk>>, Observable<ToolCallsMessage>>;
/**
 * Recovers a complete tool call message from individual chunks.
 *
 * @param toolCallMessages - Array of tool call message chunks
 * @returns A complete tool call message with all chunks combined
 */
declare function recoverToolCallMessage(toolCallMessages: ToolCallsMessage[]): ToolCallsMessage;

type llm_ChatCompletionsChunk = ChatCompletionsChunk;
type llm_ChatCompletionsDelta = ChatCompletionsDelta;
type llm_ChatCompletionsRequest = ChatCompletionsRequest;
type llm_ChatCompletionsResponse<T = Choice> = ChatCompletionsResponse<T>;
type llm_Choice = Choice;
type llm_ContentMessage = ContentMessage;
type llm_DoneMessage = DoneMessage;
type llm_Function = Function;
type llm_FunctionCallMessage = FunctionCallMessage;
type llm_LLMStreamState = LLMStreamState;
type llm_Message = Message;
type llm_Model = Model;
declare const llm_Model: typeof Model;
type llm_Role = Role;
type llm_StreamStatus = StreamStatus;
declare const llm_StreamStatus: typeof StreamStatus;
declare const llm_TIMEOUT: typeof TIMEOUT;
type llm_Tool = Tool;
type llm_ToolCall = ToolCall;
type llm_ToolCallsMessage = ToolCallsMessage;
type llm_Usage = Usage;
declare const llm_accumulateContent: typeof accumulateContent;
declare const llm_accumulateToolCalls: typeof accumulateToolCalls;
declare const llm_chatCompletions: typeof chatCompletions;
declare const llm_extractContent: typeof extractContent;
declare const llm_isContentMessage: typeof isContentMessage;
declare const llm_isDoneMessage: typeof isDoneMessage;
declare const llm_isErrorResponse: typeof isErrorResponse;
declare const llm_isFunctionCallMessage: typeof isFunctionCallMessage;
declare const llm_isToolCallsMessage: typeof isToolCallsMessage;
declare const llm_recoverToolCallMessage: typeof recoverToolCallMessage;
declare const llm_streamChatCompletions: typeof streamChatCompletions;
declare const llm_useLLMStream: typeof useLLMStream;
declare namespace llm {
  export { type llm_ChatCompletionsChunk as ChatCompletionsChunk, type llm_ChatCompletionsDelta as ChatCompletionsDelta, type llm_ChatCompletionsRequest as ChatCompletionsRequest, type llm_ChatCompletionsResponse as ChatCompletionsResponse, type llm_Choice as Choice, type llm_ContentMessage as ContentMessage, type llm_DoneMessage as DoneMessage, type llm_Function as Function, type llm_FunctionCallMessage as FunctionCallMessage, type llm_LLMStreamState as LLMStreamState, type llm_Message as Message, llm_Model as Model, type llm_Role as Role, llm_StreamStatus as StreamStatus, llm_TIMEOUT as TIMEOUT, type llm_Tool as Tool, type llm_ToolCall as ToolCall, type llm_ToolCallsMessage as ToolCallsMessage, type llm_Usage as Usage, llm_accumulateContent as accumulateContent, llm_accumulateToolCalls as accumulateToolCalls, llm_chatCompletions as chatCompletions, enabled$2 as enabled, llm_extractContent as extractContent, health$1 as health, llm_isContentMessage as isContentMessage, llm_isDoneMessage as isDoneMessage, llm_isErrorResponse as isErrorResponse, llm_isFunctionCallMessage as isFunctionCallMessage, llm_isToolCallsMessage as isToolCallsMessage, llm_recoverToolCallMessage as recoverToolCallMessage, llm_streamChatCompletions as streamChatCompletions, llm_useLLMStream as useLLMStream };
}

/**
 * @deprecated This module is deprecated and will be removed in a future version.
 * Please use the vendor-neutral `llm.ts` module instead.
 *
 * All exports from this file are re-exported from `llm.ts` for backward compatibility.
 *
 * BREAKING CHANGE in v0.13.0: The health check response format has changed from
 * { details: { openAI: { configured: true, ok: true } } }
 * to
 * { details: { llmProvider: { configured: true, ok: true } } }
 *
 * This module now handles both formats for backward compatibility, but will be removed in a future version.
 */

declare const enabled$1: () => Promise<boolean>;

type openai_ChatCompletionsChunk = ChatCompletionsChunk;
type openai_ChatCompletionsDelta = ChatCompletionsDelta;
type openai_ChatCompletionsRequest = ChatCompletionsRequest;
type openai_ChatCompletionsResponse<T = Choice> = ChatCompletionsResponse<T>;
type openai_Choice = Choice;
type openai_ContentMessage = ContentMessage;
type openai_DoneMessage = DoneMessage;
type openai_Function = Function;
type openai_FunctionCallMessage = FunctionCallMessage;
type openai_LLMStreamState = LLMStreamState;
type openai_Message = Message;
type openai_Model = Model;
declare const openai_Model: typeof Model;
type openai_Role = Role;
type openai_StreamStatus = StreamStatus;
declare const openai_StreamStatus: typeof StreamStatus;
declare const openai_TIMEOUT: typeof TIMEOUT;
type openai_Tool = Tool;
type openai_ToolCall = ToolCall;
type openai_ToolCallsMessage = ToolCallsMessage;
type openai_Usage = Usage;
declare const openai_accumulateContent: typeof accumulateContent;
declare const openai_accumulateToolCalls: typeof accumulateToolCalls;
declare const openai_chatCompletions: typeof chatCompletions;
declare const openai_extractContent: typeof extractContent;
declare const openai_isContentMessage: typeof isContentMessage;
declare const openai_isDoneMessage: typeof isDoneMessage;
declare const openai_isErrorResponse: typeof isErrorResponse;
declare const openai_isFunctionCallMessage: typeof isFunctionCallMessage;
declare const openai_isToolCallsMessage: typeof isToolCallsMessage;
declare const openai_recoverToolCallMessage: typeof recoverToolCallMessage;
declare const openai_streamChatCompletions: typeof streamChatCompletions;
declare const openai_useLLMStream: typeof useLLMStream;
declare namespace openai {
  export { type openai_ChatCompletionsChunk as ChatCompletionsChunk, type openai_ChatCompletionsDelta as ChatCompletionsDelta, type openai_ChatCompletionsRequest as ChatCompletionsRequest, type openai_ChatCompletionsResponse as ChatCompletionsResponse, type openai_Choice as Choice, type openai_ContentMessage as ContentMessage, type openai_DoneMessage as DoneMessage, type openai_Function as Function, type openai_FunctionCallMessage as FunctionCallMessage, type openai_LLMStreamState as LLMStreamState, type openai_Message as Message, openai_Model as Model, type openai_Role as Role, openai_StreamStatus as StreamStatus, openai_TIMEOUT as TIMEOUT, type openai_Tool as Tool, type openai_ToolCall as ToolCall, type openai_ToolCallsMessage as ToolCallsMessage, type openai_Usage as Usage, openai_accumulateContent as accumulateContent, openai_accumulateToolCalls as accumulateToolCalls, openai_chatCompletions as chatCompletions, enabled$1 as enabled, openai_extractContent as extractContent, health$1 as health, openai_isContentMessage as isContentMessage, openai_isDoneMessage as isDoneMessage, openai_isErrorResponse as isErrorResponse, openai_isFunctionCallMessage as isFunctionCallMessage, openai_isToolCallsMessage as isToolCallsMessage, openai_recoverToolCallMessage as recoverToolCallMessage, openai_streamChatCompletions as streamChatCompletions, openai_useLLMStream as useLLMStream };
}

/**
 * An MCP transport which uses the Grafana LLM plugin's built-in MCP server,
 * over Grafana Live.
 *
 * Use this with a client from `@modelcontextprotocol/sdk`.
 *
 * @experimental
 */
declare class GrafanaLiveTransport implements Transport {
    _grafanaLiveSrv: GrafanaLiveSrv;
    /**
     * The Grafana Live channel used by this transport.
     */
    _subscribeChannel: LiveChannelAddress;
    /**
     * The Grafana Live channel used by this transport.
     */
    _publishChannel: LiveChannelAddress;
    /**
     * The Grafana Live stream over which MCP messages are received.
     */
    _stream?: Observable<LiveChannelMessageEvent<unknown>>;
    onclose?: (() => void) | undefined;
    onerror?: ((error: Error) => void) | undefined;
    onmessage?: ((message: JSONRPCMessage) => void) | undefined;
    constructor(path?: string);
    start(): Promise<void>;
    send(message: JSONRPCMessage): Promise<void>;
    close(): Promise<void>;
}
interface MCPClientProviderProps {
    appName: string;
    appVersion: string;
    children: React.ReactNode;
}
/**
 * MCPClientProvider is a React context provider that creates an MCP client
 * and manages its lifecycle.
 *
 * It should be used to wrap the entire application in a single provider.
 * This ensures that the client is created once and reused across the application.
 *
 * It also supports Suspense, which will suspend the component until the client
 * is ready. This allows you to use the client in components that are not yet
 * ready, such as those that are loading data.
 *
 * Example usage:
 * ```tsx
 * <Suspense fallback={<LoadingPlaceholder />}>
 *   <ErrorBoundary>
 *     {({ error }) => {
 *       if (error) {
 *         return <div>Something went wrong: {error.message}</div>;
 *       }
 *       return (
 *         <MCPClientProvider appName="MyApp" appVersion="1.0.0">
 *           <YourComponent />
 *         </MCPClientProvider>
 *       );
 *     }}
 *   </ErrorBoundary>
 * </Suspense>
 * ```
 *
 * @experimental
 */
declare function MCPClientProvider({ appName, appVersion, children, }: MCPClientProviderProps): React.JSX.Element;
/**
 * Convenience hook to use an MCP client from a component.
 *
 * This hook should be used within an `MCPClientProvider`.
 *
 * @experimental
 */
declare function useMCPClient(): Client;

/**
 * Convert an array of MCP tools to an array of OpenAI tools.
 *
 * This is useful when you want to use the MCP client with the LLM plugin's
 * `chatCompletions` or `streamChatCompletions` functions.
 *
 * @experimental
 */
declare function convertToolsToOpenAI(tools: Tool$1[]): Tool[];

declare const mcp_Client: typeof Client;
type mcp_GrafanaLiveTransport = GrafanaLiveTransport;
declare const mcp_GrafanaLiveTransport: typeof GrafanaLiveTransport;
declare const mcp_MCPClientProvider: typeof MCPClientProvider;
declare const mcp_convertToolsToOpenAI: typeof convertToolsToOpenAI;
declare const mcp_useMCPClient: typeof useMCPClient;
declare namespace mcp {
  export { mcp_Client as Client, mcp_GrafanaLiveTransport as GrafanaLiveTransport, mcp_MCPClientProvider as MCPClientProvider, mcp_convertToolsToOpenAI as convertToolsToOpenAI, mcp_useMCPClient as useMCPClient };
}

/**
 * Vector search API.
 *
 * This module can be used to interact with the vector database configured
 * in the Grafana LLM app plugin. That plugin must be installed, enabled and configured
 * in order for these functions to work.
 *
 * The {@link enabled} function can be used to check if the plugin is enabled and configured.
 */

interface SearchResultPayload extends Record<string, any> {
}
/**
 * A request to search for resources in the vector database.
 **/
interface SearchRequest {
    /**
     * The name of the collection to search in.
     **/
    collection: string;
    /** The query to search for. */
    query: string;
    /**
     * Limit the number of results returned to the top `topK` results.
     *
     * Defaults to 10.
     **/
    topK?: number;
    /** Metadata filters to apply to the vector search. */
    filter?: Record<string, any>;
}
/**
 * The results of a vector search.
 *
 * Results will be ordered by score, descending.
 */
interface SearchResult<T extends SearchResultPayload> {
    /**
     * The payload of the result.
     *
     * The type of this payload depends on the collection that was searched in.
     * Grafana core types will be added to the same module as this type as they
     * are implemented.
     **/
    payload: T;
    /**
     * The score of the result.
     *
     * This is a number between 0 and 1, where 1 is the best possible match.
     */
    score: number;
}
/**
 * Search for resources in the configured vector database.
 */
declare function search<T extends SearchResultPayload>(request: SearchRequest): Promise<Array<SearchResult<T>>>;
/** Check if the vector API is enabled and configured via the LLM plugin. */
declare const health: () => Promise<VectorHealthDetails>;
declare const enabled: () => Promise<boolean>;

type vector_SearchRequest = SearchRequest;
type vector_SearchResult<T extends SearchResultPayload> = SearchResult<T>;
declare const vector_enabled: typeof enabled;
declare const vector_health: typeof health;
declare const vector_search: typeof search;
declare namespace vector {
  export { type vector_SearchRequest as SearchRequest, type vector_SearchResult as SearchResult, vector_enabled as enabled, vector_health as health, vector_search as search };
}

export { llm, mcp, openai, vector };
